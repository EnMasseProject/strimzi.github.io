<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#overview">1. Overview</a></li>
<li><a href="#getting_started">2. Getting started</a>
<ul class="sectlevel2">
<li><a href="#downloading_strimzi">2.1. Downloading Strimzi</a></li>
<li><a href="#prerequisites">2.2. Prerequisites</a></li>
<li><a href="#cluster_operator">2.3. Cluster Operator</a>
<ul class="sectlevel3">
<li><a href="#deploying_to_kubernetes">2.3.1. Deploying to Kubernetes</a></li>
<li><a href="#deploying_to_openshift">2.3.2. Deploying to OpenShift</a></li>
</ul>
</li>
<li><a href="#kafka_broker">2.4. Kafka broker</a>
<ul class="sectlevel3">
<li><a href="#deploying_to_kubernetes_2">2.4.1. Deploying to Kubernetes</a></li>
<li><a href="#deploying_to_openshift_2">2.4.2. Deploying to OpenShift</a></li>
</ul>
</li>
<li><a href="#kafka_connect">2.5. Kafka Connect</a>
<ul class="sectlevel3">
<li><a href="#deploying_to_kubernetes_3">2.5.1. Deploying to Kubernetes</a></li>
<li><a href="#deploying_to_openshift_3">2.5.2. Deploying to OpenShift</a></li>
<li><a href="#using_kafka_connect_with_additional_plugins">2.5.3. Using Kafka Connect with additional plugins</a>
<ul class="sectlevel4">
<li><a href="#create_a_new_image_based_on_our_base_image">Create a new image based on our base image</a></li>
<li><a href="#using_openshift_build_and_s2i_image">Using OpenShift Build and S2I image</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#topic_operator">2.6. Topic Operator</a>
<ul class="sectlevel3">
<li><a href="#deploying_through_the_cluster_operator">2.6.1. Deploying through the Cluster Operator</a></li>
<li><a href="#deploying_standalone_topic_operator">2.6.2. Deploying standalone Topic Operator</a>
<ul class="sectlevel4">
<li><a href="#deploying_to_kubernetes_4">Deploying to Kubernetes</a></li>
<li><a href="#deploying_to_openshift_4">Deploying to OpenShift</a></li>
</ul>
</li>
<li><a href="#topic_configmap">2.6.3. Topic ConfigMap</a></li>
<li><a href="#logging">2.6.4. Logging</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#cluster_operator_2">3. Cluster Operator</a>
<ul class="sectlevel2">
<li><a href="#reconciliation">3.1. Reconciliation</a></li>
<li><a href="#kafka_assembly_details">3.2. Format of the <code>Kafka</code> resource</a>
<ul class="sectlevel3">
<li><a href="#kafka_config_map_details">3.2.1. Kafka</a>
<ul class="sectlevel4">
<li><a href="#kafka_configuration_json_config">Kafka Configuration</a></li>
<li><a href="#zookeeper_configuration_json_config">Zookeeper Configuration</a></li>
<li><a href="#storage_configuration_json_config">Storage</a></li>
<li><a href="#metrics">Metrics</a></li>
<li><a href="#logging_2">Logging</a></li>
<li><a href="#resources_json_config">Resource limits and requests</a></li>
<li><a href="#jvm_json_config">JVM Options</a></li>
<li><a href="#kafka_rack">Kafka rack</a></li>
<li><a href="#affinity">Node and Pod Affinity</a></li>
<li><a href="#topic_operator_json_config">Topic Operator</a></li>
</ul>
</li>
<li><a href="#kafka_connect_config_map_details">3.2.2. Kafka Connect</a>
<ul class="sectlevel4">
<li><a href="#kafka_connect_configuration_json_config">Kafka Connect configuration</a></li>
<li><a href="#logging_3">Logging</a></li>
<li><a href="#kafka_connect_s2i_deployment">Kafka Connect S2I deployment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#provisioning-rbac-for-the-cluster-operator">3.3. Provisioning Role-Based Access Control (RBAC) for the Cluster Operator</a>
<ul class="sectlevel3">
<li><a href="#delegated-privileges">3.3.1. Delegated privileges</a></li>
<li><a href="#using_a_serviceaccount">3.3.2. Using a <code>ServiceAccount</code></a></li>
<li><a href="#defining_clusterroles">3.3.3. Defining <code>ClusterRoles</code></a></li>
<li><a href="#defining_clusterrolebindings">3.3.4. Defining <code>ClusterRoleBindings</code></a></li>
</ul>
</li>
<li><a href="#operator_configuration">3.4. Operator configuration</a>
<ul class="sectlevel3">
<li><a href="#multi-namespace">3.4.1. Watching multiple namespaces</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#topic_operator_2">4. Topic Operator</a>
<ul class="sectlevel2">
<li><a href="#reconciliation_2">4.1. Reconciliation</a></li>
<li><a href="#usage_recommendations">4.2. Usage Recommendations</a></li>
<li><a href="#topic_config_map_details">4.3. Format of the ConfigMap</a></li>
<li><a href="#example">4.4. Example</a>
<ul class="sectlevel3">
<li><a href="#on_kubernetes">4.4.1. On Kubernetes</a></li>
<li><a href="#on_openshift">4.4.2. On OpenShift</a></li>
</ul>
</li>
<li><a href="#unsupported_operations">4.5. Unsupported operations</a></li>
<li><a href="#operator_environment">4.6. Operator environment</a></li>
<li><a href="#resource_limits_and_requests">4.7. Resource limits and requests</a>
<ul class="sectlevel3">
<li><a href="#minimum_resource_requirements_2">4.7.1. Minimum Resource Requirements</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#security">5. Security</a>
<ul class="sectlevel2">
<li><a href="#certificates">5.1. Certificates</a></li>
<li><a href="#kafka_listeners">5.2. Kafka Listeners</a></li>
<li><a href="#kafka_client_connections_via_tls">5.3. Kafka Client connections via TLS</a></li>
</ul>
</li>
<li><a href="#frequently_asked_questions">Appendix A: Frequently Asked Questions</a>
<ul class="sectlevel2">
<li><a href="#cluster_operator_3">A.1. Cluster Operator</a>
<ul class="sectlevel3">
<li><a href="#log_contains_warnings_about_failing_to_acquire_lock">A.1.1. Log contains warnings about failing to acquire lock</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#installing_kubernetes_and_openshift_cluster">Appendix B: Installing OpenShift or Kubernetes cluster</a>
<ul class="sectlevel2">
<li><a href="#kubernetes">B.1. Kubernetes</a></li>
<li><a href="#openshift">B.2. OpenShift</a></li>
</ul>
</li>
<li><a href="#api_reference">Appendix C: Custom Resource API Reference</a>
<ul class="sectlevel2">
<li><a href="#type-KafkaAssembly">C.1. <code>Kafka</code> kind v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-KafkaAssemblySpec">C.2. <code>KafkaAssemblySpec</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-Kafka">C.3. <code>Kafka</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-EphemeralStorage">C.4. <code>EphemeralStorage</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-PersistentClaimStorage">C.5. <code>PersistentClaimStorage</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-Probe">C.6. <code>Probe</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-JvmOptions">C.7. <code>JvmOptions</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-Sidecar">C.8. <code>Sidecar</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-Resources">C.9. <code>Resources</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-CpuMemory">C.10. <code>CpuMemory</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-InlineLogging">C.11. <code>InlineLogging</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-ExternalLogging">C.12. <code>ExternalLogging</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-Rack">C.13. <code>Rack</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-Zookeeper">C.14. <code>Zookeeper</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-TopicOperator">C.15. <code>TopicOperator</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-KafkaConnectAssembly">C.16. <code>KafkaConnect</code> kind v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-KafkaConnectAssemblySpec">C.17. <code>KafkaConnectAssemblySpec</code> type v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-KafkaConnectS2IAssembly">C.18. <code>KafkaConnectS2I</code> kind v1alpha1 kafka.strimzi.io</a></li>
<li><a href="#type-KafkaConnectS2IAssemblySpec">C.19. <code>KafkaConnectS2IAssemblySpec</code> type v1alpha1 kafka.strimzi.io</a></li>
</ul>
</li>
<li><a href="#metrics_2">Appendix D: Metrics</a>
<ul class="sectlevel2">
<li><a href="#deploying_on_openshift">D.1. Deploying on OpenShift</a>
<ul class="sectlevel3">
<li><a href="#prometheus">D.1.1. Prometheus</a></li>
<li><a href="#grafana">D.1.2. Grafana</a></li>
</ul>
</li>
<li><a href="#deploying_on_kubernetes">D.2. Deploying on Kubernetes</a>
<ul class="sectlevel3">
<li><a href="#prometheus_2">D.2.1. Prometheus</a></li>
<li><a href="#grafana_2">D.2.2. Grafana</a></li>
</ul>
</li>
<li><a href="#grafana_dashboard">D.3. Grafana dashboard</a></li>
</ul>
</li>
</ul>
</div>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Strimzi provides a way to run an Apache Kafka cluster on OpenShift Origin or Kubernetes in various deployment configurations.
This guide describes how to install and use Strimzi.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="overview">1. Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka is a popular platform for streaming data delivery and processing. For more details about Apache Kafka
itself visit <a href="http://kafka.apache.org">Apache Kafka website</a>. The aim of Strimzi is to make it easy to run
Apache Kafka on OpenShift or Kubernetes.
Strimzi is based on Apache Kafka 1.1.0.</p>
</div>
<div class="paragraph">
<p>Strimzi consists of two main components:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Cluster Operator</dt>
<dd>
<p>Responsible for deploying and managing Apache Kafka clusters within OpenShift or Kubernetes cluster.</p>
</dd>
<dt class="hdlist1">Topic Operator</dt>
<dd>
<p>Responsible for managing Kafka topics within a Kafka cluster running within OpenShift or Kubernetes cluster.</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect1">
<h2 id="getting_started">2. Getting started</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="downloading_strimzi">2.1. Downloading Strimzi</h3>
<div class="paragraph">
<p>Strimzi releases are available for download from <a href="https://github.com/strimzi/strimzi-kafka-operator/releases">GitHub</a>.
The release artifacts contain documentation and example YAML files for deployment on OpenShift or Kubernetes.
The example files are used throughout this documentation and can be used to install Strimzi.
The Docker images are available on <a href="https://hub.docker.com/u/strimzi">Docker Hub</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="prerequisites">2.2. Prerequisites</h3>
<div class="paragraph">
<p>Strimzi runs on OpenShift or Kubernetes.
Strimzi supports
Kubernetes 1.9 and higher or
OpenShift Origin 3.9 and higher.
Strimzi works on all kinds of clusters - from public and private clouds down to local deployments intended for development.
This guide expects that an OpenShift or Kubernetes cluster is available and the
<code>kubectl</code> or
<code>oc</code> command line tools are installed and configured to connect to the running cluster.</p>
</div>
<div class="paragraph">
<p>When no existing OpenShift or Kubernetes cluster is available, <code>Minikube</code> or <code>Minishift</code> can be used to create a local
cluster. More details can be found in <a href="#installing_kubernetes_and_openshift_cluster">Installing OpenShift or Kubernetes cluster</a></p>
</div>
<div class="paragraph">
<p>In order to execute the commands in this guide, your OpenShift or Kubernetes user needs to have the rights to create and
manage RBAC resources (Roles and Role Bindings).</p>
</div>
</div>
<div class="sect2">
<h3 id="cluster_operator">2.3. Cluster Operator</h3>
<div class="paragraph">
<p>Strimzi uses a component called the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect
clusters.
The Cluster Operator is deployed as a process running inside your OpenShift or Kubernetes cluster.
To deploy a Kafka cluster, a <code>Kafka</code> resource with the cluster configuration has to be created within the OpenShift or Kubernetes cluster.
Based on the information in that <code>Kafka</code> resource,
the Cluster Operator will deploy a corresponding Kafka cluster.
The format of the <code>Kafka</code> resource is described in <a href="#kafka_assembly_details">Format of the <code>Kafka</code> resource</a>.</p>
</div>
<div class="paragraph">
<p>Strimzi contains example YAML files which make deploying a Cluster Operator easier.</p>
</div>
<div class="sect3">
<h4 id="deploying_to_kubernetes">2.3.1. Deploying to Kubernetes</h4>
<div class="paragraph">
<p>To deploy the Cluster Operator on Kubernetes, you will first need to modify some of of the installation files according to the namespace the Cluster Operator is going to be installed in.
To do this, execute the following command, replacing <code>&lt;my-namespace&gt;</code> with the correct namespace:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">sed -i 's/namespace: .*/namespace: &lt;my-namespace&gt;/' examples/install/cluster-operator/*ClusterRoleBinding*.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, the following commands should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl create -f examples/install/cluster-operator</code></pre>
</div>
</div>
<div class="paragraph">
<p>To verify whether the Cluster Operator has been deployed successfully, the Kubernetes Dashboard or the following
command can be used:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl describe all</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="deploying_to_openshift">2.3.2. Deploying to OpenShift</h4>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
To successfully deploy the Cluster Operator on OpenShift a user with <code>cluster-admin</code> role needs to be used (that is, like <code>system:admin</code>).
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To deploy the Cluster Operator on OpenShift, you will first need to modify some of of the installation files according to the namespace the Cluster Operator is going to be installed in.
To do this, execute the following command, replacing <code>&lt;my-namespace&gt;</code> with the correct namespace:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">sed -i 's/namespace: .*/namespace: &lt;my-namespace&gt;/' examples/install/cluster-operator/*ClusterRoleBinding*.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, the following commands should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f examples/install/cluster-operator
oc create -f examples/templates/cluster-operator</code></pre>
</div>
</div>
<div class="paragraph">
<p>To verify whether the Cluster Operator has been deployed successfully, the OpenShift console or the following command
can be used:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc describe all</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="kafka_broker">2.4. Kafka broker</h3>
<div class="paragraph">
<p>Strimzi uses the StatefulSets feature of OpenShift or Kubernetes to deploy Kafka brokers.
With StatefulSets, the pods receive a unique name and network identity and that makes it easier to identify the
individual Kafka broker pods and set their identity (broker ID). The deployment uses <strong>regular</strong> and <strong>headless</strong>
services:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>regular services can be used as bootstrap servers for Kafka clients</p>
</li>
<li>
<p>headless services are needed to have DNS resolve the pods IP addresses directly</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As well as Kafka, Strimzi also installs a Zookeeper cluster and configures the Kafka brokers to connect to it. The
Zookeeper cluster also uses StatefulSets.</p>
</div>
<div class="paragraph">
<p>Strimzi provides two flavors of Kafka broker deployment: <strong>ephemeral</strong> and <strong>persistent</strong>.</p>
</div>
<div class="paragraph">
<p>The <strong>ephemeral</strong> flavour is suitable only for development and testing purposes and not for production. The
ephemeral flavour uses <code>emptyDir</code> volumes for storing broker information (Zookeeper) and topics/partitions
(Kafka). Using <code>emptyDir</code> volume means that its content is strictly related to the pod life cycle (it is
deleted when the pod goes down). This makes the in-memory deployment well-suited to development and testing because
you do not have to provide persistent volumes.</p>
</div>
<div class="paragraph">
<p>The <strong>persistent</strong> flavour uses PersistentVolumes to store Zookeeper and Kafka data. The PersistentVolume is
acquired using a PersistentVolumeClaim – that makes it independent of the actual type of the PersistentVolume. For
example, it can use
HostPath volumes on Minikube or
Amazon EBS volumes in Amazon AWS deployments without any changes in the YAML files. The PersistentVolumeClaim can use
a StorageClass to trigger automatic volume provisioning.</p>
</div>
<div class="paragraph">
<p>To deploy a Kafka cluster, a <code>Kafka</code> resource with the cluster configuration has to be created.
Example resources and the details about the <code>Kafka</code> format are in <a href="#kafka_assembly_details">Format of the <code>Kafka</code> resource</a>.</p>
</div>
<div class="sect3">
<h4 id="deploying_to_kubernetes_2">2.4.1. Deploying to Kubernetes</h4>
<div class="paragraph">
<p>To deploy a Kafka broker on Kubernetes, the corresponding <code>Kafka</code> has to be created.
To create an ephemeral cluster using the provided example <code>Kafka</code>, the following command should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -f examples/kafka/kafka-ephemeral.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Another example <code>Kafka</code> is provided for a persistent Kafka cluster.
To deploy it, the following command should be run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -f examples/kafka/kafka-persistent.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="deploying_to_openshift_2">2.4.2. Deploying to OpenShift</h4>
<div class="paragraph">
<p>For OpenShift, the Kafka broker is provided in the form of a template.
The cluster can be deployed from the template either using the command line or using the OpenShift console.
To create the ephemeral cluster, the following command should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-app strimzi-ephemeral</code></pre>
</div>
</div>
<div class="paragraph">
<p>Similarly, to deploy a persistent Kafka cluster the following command should be run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-app strimzi-persistent</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="kafka_connect">2.5. Kafka Connect</h3>
<div class="paragraph">
<p>The Cluster Operator can also deploy a <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a> cluster which can be used with either of the Kafka broker deployments described above.
It is implemented as a Deployment with a configurable number of workers.
The default image currently contains only the Connectors distributed with Apache Kafka Connect: <code>FileStreamSinkConnector</code> and <code>FileStreamSourceConnector</code>.
The REST interface for managing the Kafka Connect cluster is exposed internally within the OpenShift or Kubernetes cluster as <code>kafka-connect</code> service on port <code>8083</code>.</p>
</div>
<div class="paragraph">
<p>Example <code>KafkaConnect</code> resources and the details about the <code>KafkaConnect</code> format for deploying Kafka Connect can be found in
<a href="#kafka_connect_config_map_details">Kafka Connect</a>.</p>
</div>
<div class="sect3">
<h4 id="deploying_to_kubernetes_3">2.5.1. Deploying to Kubernetes</h4>
<div class="paragraph">
<p>To deploy Kafka Connect on Kubernetes, the corresponding <code>KafkaConnect</code> resource has to be created.
An example resource can be created using the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -f examples/kafka-connect/kafka-connect.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="deploying_to_openshift_3">2.5.2. Deploying to OpenShift</h4>
<div class="paragraph">
<p>On OpenShift, Kafka Connect is provided in the form of a template. It can be deployed from the template either using the command line or using the OpenShift console.
To create a Kafka Connect cluster from the command line, the following command should be run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-app strimzi-connect</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="using_kafka_connect_with_additional_plugins">2.5.3. Using Kafka Connect with additional plugins</h4>
<div class="paragraph">
<p>Strimzi Docker images for Kafka Connect contain, by default, only the <code>FileStreamSinkConnector</code> and <code>FileStreamSourceConnector</code> connectors which are part of Apache Kafka.</p>
</div>
<div class="paragraph">
<p>To facilitate deployment with 3rd party connectors, Kafka Connect is configured to automatically load all plugins/connectors which are present in the <code>/opt/kafka/plugins</code> directory during startup.
There are two ways of adding custom plugins into this directory:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Using a custom Docker image</p>
</li>
<li>
<p>Using the OpenShift build system with the Strimzi S2I image</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="create_a_new_image_based_on_our_base_image">Create a new image based on our base image</h5>
<div class="paragraph">
<p>Strimzi provides its own Docker image for running Kafka Connect which can be found on <a href="https://hub.docker.com/u/strimzi">Docker Hub</a> as
<code>strimzi/kafka-connect:latest</code>.
This image could be used as a base image for building a new custom image with additional plugins.
The following steps describe the process for creating such a custom image:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a new <code>Dockerfile</code> which uses <code>strimzi/kafka-connect:latest</code> as the base image</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-Dockerfile hljs" data-lang="Dockerfile">FROM strimzi/kafka-connect:latest
USER root:root
COPY ./my-plugin/ /opt/kafka/plugins/
USER kafka:kafka</code></pre>
</div>
</div>
</li>
<li>
<p>Build the Docker image and upload it to the appropriate Docker repository</p>
</li>
<li>
<p>Use the new Docker image in the Kafka Connect deployment:</p>
<div class="ulist">
<ul>
<li>
<p>On OpenShift, the template parameters <code>IMAGE_REPO_NAME</code>, <code>IMAGE_NAME</code> and <code>IMAGE_TAG</code> can be changed to point to the new image when the Kafka Connect cluster is being deployed</p>
</li>
<li>
<p>On Kubernetes, the KafkaConnect resource has to be modified to use the new image</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="using_openshift_build_and_s2i_image">Using OpenShift Build and S2I image</h5>
<div class="paragraph">
<p>OpenShift supports <a href="https://docs.openshift.org/3.9/dev_guide/builds/index.html">Builds</a> which can be used together with the <a href="https://docs.openshift.org/3.9/creating_images/s2i.html#creating-images-s2i">Source-to-Image (S2I)</a> framework to create new Docker images.
OpenShift Build takes a builder image with S2I support together with source code and binaries provided by the user and uses them to build a new Docker image.
The newly created Docker Image will be stored in OpenShift&#8217;s local Docker repository and can then be used in deployments.
Strimzi provides a Kafka Connect builder image which can be found on <a href="https://hub.docker.com/u/strimzi">Docker Hub</a> as <code>strimzi/kafka-connect-s2i:latest</code> with such S2I support.
It takes user-provided binaries (with plugins and connectors) and creates a new Kafka Connect image.
This enhanced Kafka Connect image can be used with our Kafka Connect deployment.</p>
</div>
<div class="paragraph">
<p>The S2I deployment is again provided as an OpenShift template. It can be deployed from the template either using the command
line or using the OpenShift console. To create Kafka Connect S2I cluster from the command line, the following command should
be run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc new-app strimzi-connect-s2i</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once the cluster is deployed, a new Build can be triggered from the command line:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A directory with Kafka Connect plugins has to be prepared first. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ tree ./my-plugins/
./my-plugins/
├── debezium-connector-mongodb
│   ├── bson-3.4.2.jar
│   ├── CHANGELOG.md
│   ├── CONTRIBUTE.md
│   ├── COPYRIGHT.txt
│   ├── debezium-connector-mongodb-0.7.1.jar
│   ├── debezium-core-0.7.1.jar
│   ├── LICENSE.txt
│   ├── mongodb-driver-3.4.2.jar
│   ├── mongodb-driver-core-3.4.2.jar
│   └── README.md
├── debezium-connector-mysql
│   ├── CHANGELOG.md
│   ├── CONTRIBUTE.md
│   ├── COPYRIGHT.txt
│   ├── debezium-connector-mysql-0.7.1.jar
│   ├── debezium-core-0.7.1.jar
│   ├── LICENSE.txt
│   ├── mysql-binlog-connector-java-0.13.0.jar
│   ├── mysql-connector-java-5.1.40.jar
│   ├── README.md
│   └── wkb-1.0.2.jar
└── debezium-connector-postgres
    ├── CHANGELOG.md
    ├── CONTRIBUTE.md
    ├── COPYRIGHT.txt
    ├── debezium-connector-postgres-0.7.1.jar
    ├── debezium-core-0.7.1.jar
    ├── LICENSE.txt
    ├── postgresql-42.0.0.jar
    ├── protobuf-java-2.6.1.jar
    └── README.md</code></pre>
</div>
</div>
</li>
<li>
<p>To start a new image build using the prepared directory, the following command has to be run:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc start-build my-connect-cluster-connect --from-dir ./my-plugins/</code></pre>
</div>
</div>
<div class="paragraph">
<p><em>The name of the build should be changed according to the cluster name of the deployed Kafka Connect cluster.</em></p>
</div>
</li>
<li>
<p>Once the build is finished, the new image will be used automatically by the Kafka Connect deployment.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="topic_operator">2.6. Topic Operator</h3>
<div class="paragraph">
<p>Strimzi uses a component called the Topic Operator to manage topics in the Kafka cluster. The Topic Operator
is deployed as a process running inside a OpenShift or Kubernetes cluster. To create a new Kafka topic, a ConfigMap
with the related configuration (name, partitions, replication factor, &#8230;&#8203;) has to be created. Based on the information
in that ConfigMap, the Topic Operator will create a corresponding Kafka topic in the cluster.</p>
</div>
<div class="paragraph">
<p>Deleting a topic ConfigMap causes the deletion of the corresponding Kafka topic as well.</p>
</div>
<div class="paragraph">
<p>The Cluster Operator is able to deploy a Topic Operator, which can be configured in the <code>Kafka</code> resource.
Alternatively, it is possible to deploy a Topic Operator manually, rather than having it deployed
by the Cluster Operator.</p>
</div>
<div class="sect3">
<h4 id="deploying_through_the_cluster_operator">2.6.1. Deploying through the Cluster Operator</h4>
<div class="paragraph">
<p>To deploy the Topic Operator through the Cluster Operator, its configuration needs to be provided in the
<code>Kafka</code> resource in the <code>topicOperator</code> field as a JSON string.</p>
</div>
<div class="paragraph">
<p>For more information on the JSON configuration format see <a href="#topic_operator_json_config">Topic Operator</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="deploying_standalone_topic_operator">2.6.2. Deploying standalone Topic Operator</h4>
<div class="paragraph">
<p>If you are not going to deploy the Kafka cluster using the Cluster Operator but you already have a Kafka cluster deployed
on OpenShift or Kubernetes, it could be useful to deploy the Topic Operator using the provided YAML files.
In that case you can still leverage on the Topic Operator features of managing Kafka topics through related ConfigMaps.</p>
</div>
<div class="sect4">
<h5 id="deploying_to_kubernetes_4">Deploying to Kubernetes</h5>
<div class="paragraph">
<p>To deploy the Topic Operator on Kubernetes (not through the Cluster Operator), the following command should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl create -f examples/install/topic-operator.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>To verify whether the Topic Operator has been deployed successfully, the Kubernetes Dashboard or the following
command can be used:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl describe all</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="deploying_to_openshift_4">Deploying to OpenShift</h5>
<div class="paragraph">
<p>To deploy the Topic Operator on OpenShift (not through the Cluster Operator), the following command should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f examples/install/topic-operator</code></pre>
</div>
</div>
<div class="paragraph">
<p>To verify whether the Topic Operator has been deployed successfully, the OpenShift console or the following command
can be used:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc describe all</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="topic_configmap">2.6.3. Topic ConfigMap</h4>
<div class="paragraph">
<p>When the Topic Operator is deployed by the Cluster Operator it will be configured to watch
for "topic ConfigMaps" which are those with the following labels:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">strimzi.io/cluster: &lt;cluster-name&gt;
strimzi.io/kind: topic</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
When the Topic Operator is deployed manually the <code>strimzi.io/cluster</code> label is not necessary.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The topic ConfigMap contains the topic configuration in a specific format. The ConfigMap format is described in <a href="#topic_config_map_details">Format of the ConfigMap</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="logging">2.6.4. Logging</h4>
<div class="paragraph">
<p>The <code>logging</code> field allows the configuration of loggers. These loggers are listed below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>rootLogger.level</code></pre>
</div>
</div>
<div class="paragraph">
<p>For information on the logging options and examples of how to set logging, see <a href="#logging_examples">logging examples</a> for Kafka.</p>
</div>
<div class="paragraph">
<p>When using external ConfigMap remember to place your custom ConfigMap under <code>log4j2.properties</code> key.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="cluster_operator_2">3. Cluster Operator</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Cluster Operator is in charge of deploying a Kafka cluster alongside a Zookeeper ensemble.
As part of the Kafka cluster, it can also deploy the topic operator which provides operator-style topic management via ConfigMaps.
The Cluster Operator is also able to deploy a Kafka Connect cluster which connects to an existing Kafka cluster.
On OpenShift such a cluster can be deployed using the Source2Image feature, providing an easy way of including more connectors.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/cluster_operator.png" alt="Cluster Operator">
</div>
<div class="title">Figure 1. Example Architecture diagram of the Cluster Operator.</div>
</div>
<div class="paragraph">
<p>When the Cluster Operator is up, it starts to "watch" for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration.
A <code>Kafka</code> resource is used for Kafka cluster configuration, and a <code>KafkaConnect</code> resource is used for Kafka Connect cluster configuration.</p>
</div>
<div class="paragraph">
<p>When a new desired resource (that is, a <code>Kafka</code> or <code>KafkaConnect</code> resource) is created in the OpenShift or Kubernetes cluster, the operator gets the cluster configuration from the desired resource and starts creating a new Kafka or Kafka Connect cluster by creating the necessary other OpenShift or Kubernetes resources, such as StatefulSets, Services, ConfigMaps, and so on.</p>
</div>
<div class="paragraph">
<p>Every time the desired resource is updated by the user, the operator performs corresponding updates on the OpenShift or Kubernetes resources which make up the Kafka or Kafka Connect cluster.
Resources are either patched or deleted and then re-created in order to make the Kafka or Kafka Connect cluster reflect the state of the desired cluster resource.
This might cause a rolling update which might lead to service disruption.</p>
</div>
<div class="paragraph">
<p>Finally, when the desired resource is deleted, the operator starts to un-deploy the cluster deleting all the related OpenShift or Kubernetes resources.</p>
</div>
<div class="sect2">
<h3 id="reconciliation">3.1. Reconciliation</h3>
<div class="paragraph">
<p>Although the operator reacts to all notifications about the desired cluster resources received from the OpenShift or Kubernetes cluster,
if the operator is not running, or if a notification is not received for any reason, the desired resources will get out of sync with the state of the running OpenShift or Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>In order to handle failovers properly, a periodic reconciliation process is executed by the Cluster Operator so that it can compare the state of the desired resources with the current cluster deployments in order to have a consistent state across all of them.</p>
</div>
</div>
<div class="sect2">
<h3 id="kafka_assembly_details">3.2. Format of the <code>Kafka</code> resource</h3>
<div class="paragraph">
<p>The full API is described in <a href="#kafka_resource_reference">[kafka_resource_reference]</a>.</p>
</div>
<div class="paragraph">
<p>Whatever other labels are applied to the desired Kafka resource will also be applied to the OpenShift or Kubernetes resources making up the Kafka cluster.
This provides a convenient mechanism for those resources to be labelled in whatever way the user requires.</p>
</div>
<div class="sect3">
<h4 id="kafka_config_map_details">3.2.1. Kafka</h4>
<div class="paragraph">
<p>In order to configure a Kafka cluster deployment, it is possible to specify the following fields in Kafka resource (a dot is used to denote a nested YAML object):</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>spec.kafka.replicas</code></dt>
<dd>
<p>The number of Kafka broker nodes.
Default is 3.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.image</code></dt>
<dd>
<p>The Docker image to use for the Kafka brokers.
Default is determined by the value of the <code><a href="#STRIMZI_DEFAULT_KAFKA_IMAGE">STRIMZI_DEFAULT_KAFKA_IMAGE</a></code> environment variable of the Cluster Operator.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.brokerRackInitImage</code></dt>
<dd>
<p>The Docker image to use for the init container which does some initial configuration work (that is, rack support).
Default is determined by the value of the <code><a href="#STRIMZI_DEFAULT_KAFKA_INIT_IMAGE">STRIMZI_DEFAULT_KAFKA_INIT_IMAGE</a></code> environment variable of the Cluster Operator.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.livenessProbe.initialDelaySeconds</code></dt>
<dd>
<p>The initial delay for the liveness probe for each Kafka broker node.
Default is 15.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.livenessProbe.timeoutSeconds</code></dt>
<dd>
<p>The timeout on the liveness probe for each Kafka broker node.
Default is 5.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.readinessProbe.initialDelaySeconds</code></dt>
<dd>
<p>The initial delay for the readiness probe for each Kafka broker node.
Default is 15.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.readinessProbe.timeoutSeconds</code></dt>
<dd>
<p>The timeout on the readiness probe for each Kafka broker node.
Default is 5.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.config</code></dt>
<dd>
<p>The Kafka broker configuration.
See section <a href="#kafka_configuration_json_config">Kafka Configuration</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.storage</code></dt>
<dd>
<p>The storage configuration for the Kafka broker nodes.
See section <a href="#storage_configuration_json_config">Storage</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.metrics</code></dt>
<dd>
<p>The JMX exporter configuration for exposing metrics from Kafka broker nodes.
When this field is absent no metrics will be exposed.</p>
</dd>
<dt class="hdlist1"><a id="spec.kafka.logging"></a><code>spec.kafka.logging</code></dt>
<dd>
<p>An object that specifies inline logging levels or the name of external config map that specifies the logging levels.
When this field is absent default values are used.
List of loggers which can be set:</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>kafka.root.logger.level
log4j.logger.org.I0Itec.zkclient.ZkClient
log4j.logger.org.apache.zookeeper
log4j.logger.kafka
log4j.logger.org.apache.kafka
log4j.logger.kafka.request.logger
log4j.logger.kafka.network.Processor
log4j.logger.kafka.server.KafkaApis
log4j.logger.kafka.network.RequestChannel$
log4j.logger.kafka.controller
log4j.logger.kafka.log.LogCleaner
log4j.logger.state.change.logger
log4j.logger.kafka.authorizer.logger</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>spec.kafka.resources</code></dt>
<dd>
<p>The resource limits and requests for Kafka broker containers.
The accepted format is described in the <a href="#resources_json_config">Resource limits and requests</a> section.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.jvmOptions</code></dt>
<dd>
<p>An object allowing the JVM running Kafka to be configured.
The accepted format is described in the <a href="#jvm_json_config">JVM Options</a> section.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.rack</code></dt>
<dd>
<p>An object allowing the Kafka rack feature to be configured and used in rack-aware partition assignment for fault tolerance.
The accepted JSON format is described in the <a href="#kafka_rack">Kafka rack</a> section.</p>
</dd>
<dt class="hdlist1"><code>spec.kafka.affinity</code></dt>
<dd>
<p>An object allowing control over how the Kafka pods are scheduled to nodes.
The format of the corresponding key is the same as the content supported in the Pod <code>affinity</code> in OpenShift or Kubernetes.
See section <a href="#affinity">Node and Pod Affinity</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.replicas</code></dt>
<dd>
<p>The number of Zookeeper nodes.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.image</code></dt>
<dd>
<p>The Docker image to use for the Zookeeper nodes.
Default is determined by the value of the <code><a href="#STRIMZI_DEFAULT_ZOOKEEPER_IMAGE">STRIMZI_DEFAULT_ZOOKEEPER_IMAGE</a></code> environment variable of the Cluster Operator.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.livenessProbe.initialDelaySeconds</code></dt>
<dd>
<p>The initial delay for the liveness probe for each Zookeeper node.
Default is 15.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.livenessProbe.initialDelaySeconds</code></dt>
<dd>
<p>The timeout on the liveness probe for each Zookeeper node.
Default is 5.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.readinessProbe.initialDelaySeconds</code></dt>
<dd>
<p>The initial delay for the readiness probe for each Zookeeper node.
Default is 15.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.readinessProbe.initialDelaySeconds</code></dt>
<dd>
<p>The timeout on the readiness probe for each Zookeeper node.
Default is 5.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.config</code></dt>
<dd>
<p>The Zookeeper configuration. See section <a href="#zookeeper_configuration_json_config">Zookeeper Configuration</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.storage</code></dt>
<dd>
<p>The storage configuration for the Zookeeper nodes. See section <a href="#storage_configuration_json_config">Storage</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.metrics</code></dt>
<dd>
<p>The JMX exporter configuration for exposing metrics from Zookeeper nodes.
When this field is absent no metrics will be exposed.</p>
</dd>
<dt class="hdlist1"><a id="spec.zookeeper.logging"></a><code>spec.zookeeper.logging</code></dt>
<dd>
<p>An object that specifies inline logging levels or the name of external config map that specifies the logging levels.
When this field is absent default values are used.
List of loggers which can be set:</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>zookeeper.root.logger</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>spec.zookeeper.resources</code></dt>
<dd>
<p>An object configuring the resource limits and requests for Zookeeper broker containers.
The accepted JSON format is described in the <a href="#resources_json_config">Resource limits and requests</a> section.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.jvmOptions</code></dt>
<dd>
<p>An object allowing the JVM running Zookeeper to be configured.
The accepted JSON format is described in the <a href="#jvm_json_config">JVM Options</a> section.</p>
</dd>
<dt class="hdlist1"><code>spec.zookeeper.affinity</code></dt>
<dd>
<p>An object allowing control over how the Zookeeper pods are scheduled to nodes.
The format of the corresponding key is the same as the content supported in the Pod <code>affinity</code> in OpenShift or Kubernetes.
See section <a href="#affinity">Node and Pod Affinity</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>spec.topicOperator</code></dt>
<dd>
<p>An object representing the topic operator configuration.
See the <a href="#topic_operator_json_config">Topic Operator</a> documentation for further details.
More info about the topic operator in the related <a href="#Topic operator">[Topic operator]</a> documentation page.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The following is an example of a Kafka resource.</p>
</div>
<div class="listingblock">
<div class="title">Example <code>Kafka</code> resource</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    replicas: 3
    image: "strimzi/kafka:latest"
    kafka-healthcheck-delay: "15"
    kafka-healthcheck-timeout: "5"
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: ephemeral
    metrics:
      {
        "lowercaseOutputName": true,
        "rules": [
            {
              "pattern": "kafka.server&lt;type=(.+), name=(.+)PerSec\\w*&gt;&lt;&gt;Count",
              "name": "kafka_server_$1_$2_total"
            },
            {
              "pattern": "kafka.server&lt;type=(.+), name=(.+)PerSec\\w*, topic=(.+)&gt;&lt;&gt;Count",
              "name": "kafka_server_$1_$2_total",
              "labels":
              {
                "topic": "$3"
              }
            }
        ]
      }
    logging:
      type: external
      name: customConfigMap
  zookeeper:
    replicas: 1
    image: strimzi/zookeeper:latest
    healthcheck-delay: "15"
    healthcheck-timeout: "5"
    config:
      timeTick: 2000,
      initLimit: 5,
      syncLimit: 2,
      autopurge.purgeInterval: 1
    storage:
      type: ephemeral
    metrics:
      {
        "lowercaseOutputName": true
      }
    logging:
      type : inline
      loggers :
        zookeeper.root.logger: INFO</code></pre>
</div>
</div>
<div class="paragraph">
<p>The resources created by the Cluster Operator in the OpenShift or Kubernetes cluster will be the following :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>[cluster-name]-zookeeper</code></dt>
<dd>
<p>StatefulSet which is in charge of managing the Zookeeper node pods</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-kafka</code></dt>
<dd>
<p>StatefulSet which is in charge of managing the Kafka broker pods</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-zookeeper-nodes</code></dt>
<dd>
<p>Service needed to have DNS resolve the Zookeeper pods IP addresses directly</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-kafka-brokers</code></dt>
<dd>
<p>Service needed to have DNS resolve the Kafka broker pods IP addresses directly</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-zookeeper-client</code></dt>
<dd>
<p>Service used by Kafka brokers to connect to Zookeeper nodes as clients</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-kafka-bootstrap</code></dt>
<dd>
<p>Service can be used as bootstrap servers for Kafka clients</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-zookeeper-metrics-config</code></dt>
<dd>
<p>ConfigMap which contains the Zookeeper metrics configuration and mounted as a volume by the Zookeeper node pods</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-kafka-metrics-config</code></dt>
<dd>
<p>ConfigMap which contains the Kafka metrics configuration and mounted as a volume by the Kafka broker pods</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-zookeeper-config</code></dt>
<dd>
<p>ConfigMap which contains the Zookeeper ancillary configuration and is mounted as a volume by the Zookeeper node pods</p>
</dd>
<dt class="hdlist1"><code>[cluster-name]-kafka-config</code></dt>
<dd>
<p>ConfigMap which contains the Kafka ancillary configuration and is mounted as a volume by the Kafka broker pods</p>
</dd>
</dl>
</div>
<div class="sect4">
<h5 id="kafka_configuration_json_config">Kafka Configuration</h5>
<div class="paragraph">
<p>The <code>spec.kafka.config</code> object allows detailed configuration of Apache Kafka. This field should contain a JSON object with Kafka
configuration options as keys. The values could be in one of the following JSON types:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>String</p>
</li>
<li>
<p>Number</p>
</li>
<li>
<p>Boolean</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>spec.kafka.config</code> object supports all Kafka configuration options with the exception of options related to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Security (Encryption, Authentication and Authorization)</p>
</li>
<li>
<p>Listener configuration</p>
</li>
<li>
<p>Broker ID configuration</p>
</li>
<li>
<p>Configuration of log data directories</p>
</li>
<li>
<p>Inter-broker communication</p>
</li>
<li>
<p>Zookeeper connectivity</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Specifically, all configuration options with keys starting with one of the following strings will be ignored:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>listeners</code></p>
</li>
<li>
<p><code>advertised.</code></p>
</li>
<li>
<p><code>broker.</code></p>
</li>
<li>
<p><code>listener.</code></p>
</li>
<li>
<p><code>host.name</code></p>
</li>
<li>
<p><code>port</code></p>
</li>
<li>
<p><code>inter.broker.listener.name</code></p>
</li>
<li>
<p><code>sasl.</code></p>
</li>
<li>
<p><code>ssl.</code></p>
</li>
<li>
<p><code>security.</code></p>
</li>
<li>
<p><code>password.</code></p>
</li>
<li>
<p><code>principal.builder.class</code></p>
</li>
<li>
<p><code>log.dir</code></p>
</li>
<li>
<p><code>zookeeper.connect</code></p>
</li>
<li>
<p><code>zookeeper.set.acl</code></p>
</li>
<li>
<p><code>authorizer.</code></p>
</li>
<li>
<p><code>super.user</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All other options will be passed to Kafka.
A list of all the available options can be found on the <a href="http://kafka.apache.org/11/documentation.html#brokerconfigs">Kafka website</a>.
An example <code>spec.kafka.config</code> field is provided below.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource specifying Kafka configuration</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    config:
      num.partitions: 1,
      num.recovery.threads.per.data.dir: 1,
      default.replication.factor: 3,
      offsets.topic.replication.factor: 3,
      transaction.state.log.replication.factor: 3,
      transaction.state.log.min.isr: 1,
      log.retention.hours: 168,
      log.segment.bytes: 1073741824,
      log.retention.check.interval.ms: 300000,
      num.network.threads: 3,
      num.io.threads: 8,
      socket.send.buffer.bytes: 102400,
      socket.receive.buffer.bytes: 102400,
      socket.request.max.bytes: 104857600,
      group.initial.rebalance.delay.ms: 0
    # ...</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">NOTE</dt>
<dd>
<p>The Cluster Operator does not validate keys or values in the provided <code>config</code> object.
When invalid configuration is provided, the Kafka cluster might not start or might become unstable.
In such cases, the configuration in the <code>spec.kafka.config</code> object should be fixed and the cluster operator will roll out the new configuration to all Kafka brokers.</p>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="zookeeper_configuration_json_config">Zookeeper Configuration</h5>
<div class="paragraph">
<p>The <code>spec.zookeeper.config</code> object allows detailed configuration of Apache Zookeeper. This field should contain a JSON object
with Zookeeper configuration options as keys. The values could be in one of the following JSON types:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>String</p>
</li>
<li>
<p>Number</p>
</li>
<li>
<p>Boolean</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>spec.zookeeper.config</code> object supports all Zookeeper configuration options with the exception of options related to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Security (Encryption, Authentication and Authorization)</p>
</li>
<li>
<p>Listener configuration</p>
</li>
<li>
<p>Configuration of data directories</p>
</li>
<li>
<p>Zookeeper cluster composition</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Specifically, all configuration options with keys starting with one of the following strings will be ignored:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>server.</code></p>
</li>
<li>
<p><code>dataDir</code></p>
</li>
<li>
<p><code>dataLogDir</code></p>
</li>
<li>
<p><code>clientPort</code></p>
</li>
<li>
<p><code>authProvider</code></p>
</li>
<li>
<p><code>quorum.auth</code></p>
</li>
<li>
<p><code>requireClientAuthScheme</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All other options will be passed to Zookeeper.
A list of all the available options can be found on the <a href="http://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html">Zookeeper website</a>.
An example <code>spec.zookeeper.config</code> object is provided below.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource specifying Zookeeper configuration</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  zookeeper:
    # ...
    config:
      timeTick: 2000,
      initLimit: 5,
      syncLimit: 2,
      quorumListenOnAllIPs: true,
      maxClientCnxns: 0,
      autopurge.snapRetainCount: 3,
      autopurge.purgeInterval: 1
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Selected options have default values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>timeTick</code> with default value <code>2000</code></p>
</li>
<li>
<p><code>initLimit</code> with default value <code>5</code></p>
</li>
<li>
<p><code>syncLimit</code> with default value <code>2</code></p>
</li>
<li>
<p><code>autopurge.purgeInterval</code> with default value <code>1</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These options will be automatically configured in case they are not present in the <code>spec.zookeeper.config</code> object.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">NOTE</dt>
<dd>
<p>The Cluster Operator does not validate keys or values in the provided <code>config</code> object.
When invalid configuration is provided, the Zookeeper cluster might not start or might become unstable.
In such cases, the configuration in the <code>spec.zookeeper.config</code> object should be fixed and the cluster operator will roll out the new configuration to all Zookeeper nodes.</p>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="storage_configuration_json_config">Storage</h5>
<div class="paragraph">
<p>Both Kafka and Zookeeper save data to files.</p>
</div>
<div class="paragraph">
<p>Strimzi allows to save such data in an "ephemeral" way (using <code>emptyDir</code>) or in a "persistent-claim" way using persistent volumes.
It is possible to provide the storage configuration in the <code>spec.kafka.storage</code> and <code>spec.zookeeper.storage</code> objects.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
The <code>spec.kafka.storage</code> and <code>spec.zookeeper.storage</code> objects cannot be changed when the cluster is up.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The storage object has a mandatory <code>type</code> field for specifying the type of storage to use which must be either "ephemeral" or "persistent-claim".</p>
</div>
<div class="paragraph">
<p>The "ephemeral" storage is really simple to configure.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource using <code>ephemeral</code> storage for Kafka pods</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: ephemeral
    # ...</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
If the Zookeeper cluster is deployed using "ephemeral" storage, the Kafka brokers can have problems dealing with Zookeeper node restarts which could happen via updates in the Kafka resource.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In case of "persistent-claim" type the following fields can be provided as well:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>size</code> (required)</dt>
<dd>
<p>defines the size of the persistent volume claim, for example, "1Gi".</p>
</dd>
<dt class="hdlist1"><code>class</code> (optional)</dt>
<dd>
<p>the OpenShift or Kubernetes <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">storage class</a> to use for dynamic volume allocation.</p>
</dd>
<dt class="hdlist1"><code>selector</code> (optional)</dt>
<dd>
<p>allows to select a specific persistent volume to use.
It contains a <code>matchLabels</code> field which contains key:value pairs representing labels for selecting such a volume.</p>
</dd>
<dt class="hdlist1"><code>delete-claim</code> (optional)</dt>
<dd>
<p>boolean value which specifies if the persistent volume claim has to be deleted when the cluster is undeployed.
Default is <code>false</code>.</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configuring Kafka with <code>persistent-storage</code> and 1Gi <code>size</code></div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: persistent-claim
      size: 1Gi
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following example demonstrates use of a storage class.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configuring Kafka with <code>persistent-storage</code> using a storage class</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: persistent-claim
      size: 1Gi
      class: my-storage-class
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, a <code>selector</code> can be used in order to select a specific labelled persistent volume which provides some needed features (such as an SSD)</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configuring Kafka with "match labels" selector</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    storage:
      type: persistent-claim
      size: 1Gi
      selector:
        matchLabels:
          "hdd-type": "ssd"
      deleteClaim: true
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>When the "persistent-claim" is used, other than the resources already described in the <a href="#kafka_config_map_details">Kafka</a> section, the following resources are generated :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>data-[cluster-name]-kafka-[idx]</code></dt>
<dd>
<p>Persistent Volume Claim for the volume used for storing data for the Kafka broker pod <code>[idx]</code>.</p>
</dd>
<dt class="hdlist1"><code>data-[cluster-name]-zookeeper-[idx]</code></dt>
<dd>
<p>Persistent Volume Claim for the volume used for storing data for the Zookeeper node pod <code>[idx]</code>.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>See <a href="#kafka.strimzi.io-v1alpha1-type-EphemeralStorage">[kafka.strimzi.io-v1alpha1-type-EphemeralStorage]</a> and <a href="#kafka.strimzi.io-v1alpha1-type-PersistentClaimStorage">[kafka.strimzi.io-v1alpha1-type-PersistentClaimStorage]</a> for further details.</p>
</div>
</div>
<div class="sect4">
<h5 id="metrics">Metrics</h5>
<div class="paragraph">
<p>Strimzi uses the [Prometheus JMX exporter](<a href="https://github.com/prometheus/jmx_exporter" class="bare">https://github.com/prometheus/jmx_exporter</a>) in order to expose metrics on each node.
It is possible to configure a <code>metrics</code> object in the <code>kafka</code> and <code>zookeeper</code> objects in <code>Kafka</code> resources, and likewise a <code>metrics</code> object in the <code>spec</code> of <code>KafkaConnect</code> resources.
In all cases the <code>metrics</code> object should be the configuration for the JMX exporter.
You can find more information on how to use it in the corresponding GitHub repo.</p>
</div>
<div class="paragraph">
<p>For more information about using the metrics with Prometheus and Grafana, see <a href="#_metrics">[_metrics]</a></p>
</div>
</div>
<div class="sect4">
<h5 id="logging_2"><a id="logging_examples"></a>Logging</h5>
<div class="paragraph">
<p>The <code>logging</code> field allows the configuration of loggers. These loggers for Zookeeper and Kafka are available in the <a href="#spec.zookeeper.logging"><code>spec.zookeeper.logging</code></a> and <a href="#spec.kafka.logging"><code>spec.kafka.logging</code></a> sections respectively.</p>
</div>
<div class="paragraph">
<p>The setting can be done in one of two ways. Either by specifying the loggers and their levels directly or by using a custom config map.
An example would look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  logging:
    type: inline
    loggers:
      logger.name: "INFO"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>INFO</code> can be replaced with any log4j logger level. The available logger levels are <code>INFO</code>, <code>ERROR</code>, <code>WARN</code>, <code>TRACE</code>, <code>DEBUG</code>, <code>FATAL</code> or <code>OFF</code>.
The informations about log levels can be found in the <a href="https://logging.apache.org/log4j/2.x/manual/customloglevels.html">log4j manual</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  logging:
    type: external
    name: customConfigMap</code></pre>
</div>
</div>
<div class="paragraph">
<p>When using external ConfigMap remember to place your custom ConfigMap under <code>log4j.properties</code> key.</p>
</div>
<div class="paragraph">
<p>The difference between these two options is that the latter is not validated and does not support default values.
That means the user can supply any logging configuration, even if it is incorrect.
The first option supports default values.</p>
</div>
</div>
<div class="sect4">
<h5 id="resources_json_config">Resource limits and requests</h5>
<div class="paragraph">
<p>It is possible to configure OpenShift or Kubernetes resource limits and requests on for the <code>kafka</code>, <code>zookeeper</code> and <code>topicOperator</code> objects in the <code>Kafka</code> resource and for for the <code>spec</code> object of the <code>KafkaConnect resource.
The object may have a `requests</code> and a <code>limits</code> property, each having the same schema, consisting of <code>cpu</code> and <code>memory</code> properties.
The OpenShift or Kubernetes syntax is used for the values of <code>cpu</code> and <code>memory</code>.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configuring resource limits and requests for the Kafka pods</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "1"
        memory: "2Gi"
    # ...</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>requests.memory</code></dt>
<dd>
<p>the memory request for the container, corresponding directly to <a href="https://v1-7.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"><code>spec.containers[].resources.requests.memory</code></a> setting.
OpenShift or Kubernetes will ensure the containers have at least this much memory by running the pod on a node with at
least as much free memory as all the containers require. Optional with no default.</p>
</dd>
<dt class="hdlist1"><code>requests.cpu</code></dt>
<dd>
<p>the cpu request for the container, corresponding directly to <a href="https://v1-7.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"><code>spec.containers[].resources.requests.cpu</code></a> setting.
OpenShift or Kubernetes will ensure the containers have at least this much CPU by running the pod on a node with at least
as much uncommitted CPU as all the containers require. Optional with no default.</p>
</dd>
<dt class="hdlist1"><code>limits.memory</code></dt>
<dd>
<p>the memory limit for the container, corresponding directly to <a href="https://v1-7.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"><code>spec.containers[].resources.limits.memory</code></a> setting.
OpenShift or Kubernetes will limit the containers to this much memory, potentially terminating their pod if they use more.
Optional with no default.</p>
</dd>
<dt class="hdlist1"><code>limits.cpu</code></dt>
<dd>
<p>the cpu limit for the container, corresponding directly to <a href="https://v1-7.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"><code>spec.containers[].resources.limits.cpu</code></a> setting.
OpenShift or Kubernetes will cap the containers CPU usage to this limit. Optional with no default.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>More details about resource limits and requests can be found on <a href="https://v1-7.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">Kubernetes website</a>.</p>
</div>
<div class="sect5">
<h6 id="minimum_resource_requirements">Minimum Resource Requirements</h6>
<div class="paragraph">
<p>Testing has shown that the Cluster Operator functions adequately with 256Mi of memory and 200m CPU when watching two clusters.
It is therefore recommended to use these as a minimum when configuring resource requests and not to run it with lower limits than these.
Configuring more generous limits is recommended, especially when it is controlling multiple clusters.</p>
</div>
</div>
</div>
<div class="sect4">
<h5 id="jvm_json_config">JVM Options</h5>
<div class="paragraph">
<p>It is possible to configure a subset of available JVM options on Kafka, Zookeeper and Kafka Connect containers.
The object has a property for each JVM (<code>java</code>) option which can be configured:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>-Xmx</code></dt>
<dd>
<p>The maximum heap size. See the <a href="#setting_xmx">Setting <code>-Xmx</code></a> section for further details.</p>
</dd>
<dt class="hdlist1"><code>-Xms</code></dt>
<dd>
<p>The initial heap size.
Setting the same value for initial and maximum (<code>-Xmx</code>) heap sizes avoids the JVM having to allocate memory after startup,
at the cost of possibly allocating more heap than is really needed. For Kafka and Zookeeper pods such allocation could
cause unwanted latency. For Kafka Connect avoiding over allocation may be the more important concern, especially in
distributed mode where the effects of over-allocation will be multiplied by the number of consumers.</p>
</dd>
</dl>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
The units accepted by JVM settings such as <code>-Xmx</code> and <code>-Xms</code> are those accepted by the JDK <code>java</code>
binary in the corresponding image. Accordingly, <code>1g</code> or <code>1G</code> means 1,073,741,824 bytes, and <code>Gi</code> is not a valid unit
suffix. This is in contrast to the units used for <a href="#resources_json_config">memory limits and requests</a>, which follow the
OpenShift or Kubernetes convention where <code>1G</code> means 1,000,000,000 bytes, and <code>1Gi</code> means 1,073,741,824 bytes
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configuring <code>jvmOptions</code></div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    jvmOptions:
      "-Xmx": "2g"
      "-Xms": "2g"
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the JVM will use 2 GiB (=2,147,483,648 bytes) for its heap.
Its total memory usage will be approximately 8GiB.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>-server</code></dt>
<dd>
<p>Selects the server JVM. This option can be set to true or false. Optional.</p>
</dd>
<dt class="hdlist1"><code>-XX</code></dt>
<dd>
<p>A JSON Object for configuring advanced runtime options of a JVM. Optional</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The <code>-server</code> and <code>-XX</code> options are used to configure the <code>KAFKA_JVM_PERFORMANCE_OPTS</code> option of Apache Kafka.</p>
</div>
<div class="listingblock">
<div class="title">More sophisticated example fragment of a <code>Kafka</code> resource configuring <code>jvmOptions</code></div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    "-server": true,
    "-XX":
      "UseG1GC": true,
      "MaxGCPauseMillis": 20,
      "InitiatingHeapOccupancyPercent": 35,
      "ExplicitGCInvokesConcurrent": true,
      "UseParNewGC": false</code></pre>
</div>
</div>
<div class="paragraph">
<p>The example configuration above will result in the following JVM options:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:-UseParNewGC</code></pre>
</div>
</div>
<div class="paragraph">
<p>When neither of the two options (<code>-server</code> and <code>-XX</code>) is specified, the default Apache Kafka configuration of <code>KAFKA_JVM_PERFORMANCE_OPTS</code> will be used.</p>
</div>
<div class="sect5">
<h6 id="setting_xmx">Setting <code>-Xmx</code></h6>
<div class="paragraph">
<p>The default value used for <code>-Xmx</code> depends on whether there is a <a href="#resources_json_config">memory limit</a> for the container:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If there is a memory limit, the JVM&#8217;s maximum memory will be limited according to the kind of pod (Kafka, Zookeeper,
Topic Operator) to an appropriate value less than the limit.</p>
</li>
<li>
<p>Otherwise, when there is no memory limit, the JVM&#8217;s maximum memory will be set according to the kind of pod and the
RAM available to the container.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>Setting <code>-Xmx</code> explicitly is requires some care:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The JVM&#8217;s overall memory usage will be approximately 4 × the maximum heap, as configured by <code>-Xmx</code>.</p>
</li>
<li>
<p>If <code>-Xmx</code> is set without also setting an appropriate OpenShift or Kubernetes
memory limit, it is possible that the container will be killed should the OpenShift or Kubernetes node
experience memory pressure (from other Pods running on it).</p>
</li>
<li>
<p>If <code>-Xmx</code> is set without also setting an appropriate OpenShift or Kubernetes
memory request, it is possible that the container will scheduled to a node with insufficient memory.
In this case the container will start but crash (immediately if <code>-Xms</code> is set to <code>-Xmx</code>, or some later time if not).</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>When setting <code>-Xmx</code> explicitly, it is recommended to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>set the memory request and the memory limit to the same value,</p>
</li>
<li>
<p>use a memory request that is at least 4.5 × the <code>-Xmx</code>,</p>
</li>
<li>
<p>consider setting <code>-Xms</code> to the same value as <code>-Xms</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Furthermore, containers doing lots of disk I/O (such as Kafka broker containers) will need to leave some memory available
for use as operating system page cache. On such containers, the request memory should be substantially more than the
memory used by the JVM.</p>
</div>
</div>
</div>
<div class="sect4">
<h5 id="kafka_rack">Kafka rack</h5>
<div class="paragraph">
<p>It is possible to enable Kafka rack-awareness (more information can be found on the <a href="https://kafka.apache.org/documentation/#basic_ops_racks">Kafka racks documentation</a>)
by specifying the <code>rack</code> object in the <code>spec.kafka</code> object of the <code>Kafka</code> resource.
The <code>rack</code> object has one mandatory field named <code>topologyKey</code>.
This key needs to match one of the labels assigned to the OpenShift or Kubernetes cluster nodes.
The label is used by OpenShift or Kubernetes when scheduling Kafka broker pods to nodes.
If the OpenShift or Kubernetes cluster is running on a cloud provider platform, that label should represent the availability zone where the node is running.
Usually, the nodes are labeled with <code>failure-domain.beta.kubernetes.io/zone</code> that can be easily used as <code>topologyKey</code> value.
This will have the effect of spreading the broker pods across zones, and also setting the brokers <code>broker.rack</code> configuration parameter.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configuring the <code>rack</code></div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    rack:
      topologyKey: failure-domain.beta.kubernetes.io/zone
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the <code>failure-domain.beta.kubernetes.io/zone</code> node label will be used for scheduling Kafka broker Pods.</p>
</div>
</div>
<div class="sect4">
<h5 id="affinity">Node and Pod Affinity</h5>
<div class="paragraph">
<p>Node and Pod Affinity provide a flexible mechanism to guide the scheduling of pods to nodes by OpenShift or Kubernetes.
Node affinity can be used so that broker pods are preferentially scheduled to nodes with fast disks, for example.
Similarly, pod affinity could be used to try to schedule Kafka clients on the same nodes as Kafka brokers.
More information can be found on the <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">Kubernetes node and pod affinity documentation</a>.</p>
</div>
<div class="paragraph">
<p>The format of the corresponding key is the same as the content supported in the Pod <code>affinity</code> in OpenShift or Kubernetes, that is: <code>nodeAffinity</code>, <code>podAffinity</code> and <code>podAntiAffinity</code>.</p>
</div>
<div class="listingblock">
<div class="title">Example fragment of a <code>Kafka</code> resource configured with <code>nodeAffinity</code></div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/e2e-az-name
              operator: In
              values:
              - e2e-az1
              - e2e-az2
    # ...</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
When using both <code>affinity</code> and <a href="#kafka_rack"><code>rack</code></a> be aware that <code>rack</code> uses a pod anti-affinity.
This is necessary so that broker pods are scheduled in different failure domains, as specified via the <code>topologyKey</code>.
This anti-affinity will not be present in the <code>Kafka</code> resource&#8217;s <code>affinity</code>, but is still present on the StatefulSet and thus will still be considered by the scheduler.
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="topic_operator_json_config">Topic Operator</h5>
<div class="paragraph">
<p>Alongside the Kafka cluster and the Zookeeper ensemble, the Cluster Operator can also deploy the topic operator.
In order to do that, a <code>spec.topicOperator</code> object has to be included in the <code>Kafka</code> resource.
This object contains the topic operator configuration.
Without this object, the Cluster Operator does not deploy the topic operator.
It is still possible to deploy the topic operator by creating appropriate OpenShift or Kubernetes resources.</p>
</div>
<div class="paragraph">
<p>The YAML representation of the 'topicOperator` has no mandatory fields and if the value is an empty object
(just "{ }"), the Cluster Operator will deploy the topic operator with a default configuration.</p>
</div>
<div class="paragraph">
<p>The configurable fields are the following :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>image</code></dt>
<dd>
<p>Docker image to use for the topic operator. Default is determined by the value of the
<code><a href="#STRIMZI_DEFAULT_TOPIC_operator_IMAGE">STRIMZI_DEFAULT_TOPIC_operator_IMAGE</a></code> environment variable of the Cluster Operator.</p>
</dd>
<dt class="hdlist1"><code>watchedNamespace</code></dt>
<dd>
<p>The OpenShift or Kubernetes namespace in which the topic operator watches for topic ConfigMaps. Default is the namespace
where the topic operator is running.</p>
</dd>
<dt class="hdlist1"><code>reconciliationIntervalMs</code></dt>
<dd>
<p>The interval between periodic reconciliations in milliseconds. Default is 900000 (15 minutes).</p>
</dd>
<dt class="hdlist1"><code>zookeeperSessionTimeoutMs</code></dt>
<dd>
<p>The Zookeeper session timeout in milliseconds. Default is 20000 milliseconds (20 seconds).</p>
</dd>
<dt class="hdlist1"><code>topicMetadataMaxAttempts</code></dt>
<dd>
<p>The number of attempts for getting topics metadata from Kafka. The time between each attempt is defined as an exponential
back-off. You might want to increase this value when topic creation could take more time due to its larger size (i.e.
many partitions / replicas). Default is <code>6</code>.</p>
</dd>
<dt class="hdlist1"><code>resources</code></dt>
<dd>
<p>An object configuring the resource limits and requests for the topic operator container. The accepted JSON format is
described in the <a href="#resources_json_config">Resource limits and requests</a> section.</p>
</dd>
<dt class="hdlist1"><code>affinity</code></dt>
<dd>
<p>Node and Pod affinity for the Topic Operator, as described in the <a href="#affinity">Node and Pod Affinity</a> section.
The format of the corresponding key is the same as the content supported in the Pod <code>affinity</code> in OpenShift or Kubernetes.</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="title">Example Topic Operator JSON configuration</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{ "reconciliationIntervalMs": "900000", "zookeeperSessionTimeoutMs": "20000" }</code></pre>
</div>
</div>
<div class="paragraph">
<p>More information about these configuration parameters in the related <a href="#topic_operator">Topic Operator</a> documentation page.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="kafka_connect_config_map_details">3.2.2. Kafka Connect</h4>
<div class="paragraph">
<p>The operator watches for <code>KafkaConnect</code> resource in order to find and get configuration for a Kafka Connect cluster to deploy.</p>
</div>
<div class="paragraph">
<p>The <code>KafkaConnectS2I</code> resource provides configuration for a Kafka Connect cluster deployment using Build and Source2Image features (works only with OpenShift)</p>
</div>
<div class="paragraph">
<p>Whatever other labels are applied to the <code>KafkaConnect</code> or <code>KafkaConnectS2I</code> resources will also be applied to the OpenShift or Kubernetes resources making up the Kafka Connect cluster.
This provides a convenient mechanism for those resource to be labelled
in whatever way the user requires.</p>
</div>
<div class="paragraph">
<p>The <code>KafkaConnect</code> resource supports the following within its <code>spec</code>:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>nodes</code></dt>
<dd>
<p>Number of Kafka Connect worker nodes. Default is 1.</p>
</dd>
<dt class="hdlist1"><code>image</code></dt>
<dd>
<p>The Docker image to use for the Kafka Connect workers.
Default is determined by the value of the
<code><a href="#STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE">STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE</a></code> environment variable of the Cluster Operator.
If S2I is used (only on OpenShift), then it should be the related S2I image.</p>
</dd>
<dt class="hdlist1"><code>healthcheck-delay</code></dt>
<dd>
<p>The initial delay for the liveness and readiness probes for each Kafka Connect worker node.
Default is 60.</p>
</dd>
<dt class="hdlist1"><code>healthcheck-timeout</code></dt>
<dd>
<p>The timeout on the liveness and readiness probes for each Kafka Connect worker node.
Default is 5.</p>
</dd>
<dt class="hdlist1"><code>connect-config</code></dt>
<dd>
<p>A JSON string with Kafka Connect configuration.
See section <a href="#kafka_connect_configuration_json_config">Kafka Connect configuration</a> for more details.</p>
</dd>
<dt class="hdlist1"><code>metrics-config</code></dt>
<dd>
<p>A JSON string representing the JMX exporter configuration for exposing metrics from Kafka Connect nodes.
When this field is absent no metrics will be exposed.</p>
</dd>
<dt class="hdlist1"><code>resources</code></dt>
<dd>
<p>A JSON string configuring the resource limits and requests for Kafka Connect containers.
The accepted JSON format is described in the <a href="#resources_json_config">Resource limits and requests</a> section.</p>
</dd>
<dt class="hdlist1"><code>jvmOptions</code></dt>
<dd>
<p>A JSON string allowing the JVM running Kafka Connect to be configured.
The accepted JSON format is described in the <a href="#jvm_json_config">JVM Options</a> section.</p>
</dd>
<dt class="hdlist1"><code>affinity</code></dt>
<dd>
<p>Node and Pod affinity for the Kafka Connect pods, as described in the <a href="#affinity">Node and Pod Affinity</a> section.
The format of the corresponding key is the same as the content supported in the Pod <code>affinity</code> in OpenShift or Kubernetes.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The following is an example of a <code>KafkaConnect</code> resource.</p>
</div>
<div class="listingblock">
<div class="title">Example KafkaConnect resource</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  nodes: 1
  image: strimzi/kafka-connect:latest
  readinessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  config:
    bootstrap.servers: my-cluster-kafka-bootstrap:9092</code></pre>
</div>
</div>
<div class="paragraph">
<p>The resources created by the Cluster Operator into the OpenShift or Kubernetes cluster will be the following :</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">[connect-cluster-name]-connect</dt>
<dd>
<p>Deployment which is in charge to create the Kafka Connect worker node pods.</p>
</dd>
<dt class="hdlist1">[connect-cluster-name]-connect-api</dt>
<dd>
<p>Service which exposes the REST interface for managing the Kafka Connect cluster.</p>
</dd>
<dt class="hdlist1">[connect-cluster-name]-metrics-config</dt>
<dd>
<p>ConfigMap which contains the Kafka Connect metrics configuration and is mounted as a volume by the Kafka Connect pods.</p>
</dd>
</dl>
</div>
<div class="sect4">
<h5 id="kafka_connect_configuration_json_config">Kafka Connect configuration</h5>
<div class="paragraph">
<p>The <code>spec.config</code> object of the <code>KafkaConnect</code> and <code>KafkaConnectS2I</code> resources allows detailed configuration of Apache Kafka Connect.
This object should contain the Kafka Connect configuration options as keys. The values could be in one of the following JSON types:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>String</p>
</li>
<li>
<p>Number</p>
</li>
<li>
<p>Boolean</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>config</code> object supports all Kafka Connect configuration options with the exception of options related to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Security (Encryption, Authentication and Authorization)</p>
</li>
<li>
<p>Listener / REST interface configuration</p>
</li>
<li>
<p>Plugin path configuration</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Specifically, all configuration options with keys starting with one of the following strings will be ignored:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ssl.</code></p>
</li>
<li>
<p><code>sasl.</code></p>
</li>
<li>
<p><code>security.</code></p>
</li>
<li>
<p><code>listeners</code></p>
</li>
<li>
<p><code>plugin.path</code></p>
</li>
<li>
<p><code>rest.</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All other options will be passed to Kafka Connect. A list of all the available options can be found on the
<a href="http://kafka.apache.org/11/documentation.html#connectconfigs">Kafka website</a>. An example <code>config</code> field is provided
below.</p>
</div>
<div class="listingblock">
<div class="title">Example Kafka Connect configuration</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  config:
    bootstrap.servers: my-cluster-kafka:9092
    group.id: my-connect-cluster
    offset.storage.topic: my-connect-cluster-offsets
    config.storage.topic: my-connect-cluster-configs
    status.storage.topic: my-connect-cluster-status
    key.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: true
    value.converter.schemas.enable: true
    internal.key.converter: org.apache.kafka.connect.json.JsonConverter
    internal.value.converter: org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable: false
    internal.value.converter.schemas.enable: false
    config.storage.replication.factor: 3
    offset.storage.replication.factor: 3
    status.storage.replication.factor: 3
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Selected options have default values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>group.id</code> with default value <code>connect-cluster</code></p>
</li>
<li>
<p><code>offset.storage.topic</code> with default value <code>connect-cluster-offsets</code></p>
</li>
<li>
<p><code>config.storage.topic</code> with default value <code>connect-cluster-configs</code></p>
</li>
<li>
<p><code>status.storage.topic</code> with default value <code>connect-cluster-status</code></p>
</li>
<li>
<p><code>key.converter</code> with default value <code>org.apache.kafka.connect.json.JsonConverter</code></p>
</li>
<li>
<p><code>value.converter</code> with default value <code>org.apache.kafka.connect.json.JsonConverter</code></p>
</li>
<li>
<p><code>internal.key.converter</code> with default value <code>org.apache.kafka.connect.json.JsonConverter</code></p>
</li>
<li>
<p><code>internal.value.converter</code> with default value <code>org.apache.kafka.connect.json.JsonConverter</code></p>
</li>
<li>
<p><code>internal.key.converter.schemas.enable</code> with default value <code>false</code></p>
</li>
<li>
<p><code>internal.value.converter.schemas.enable</code> with default value <code>false</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These options will be automatically configured in case they are not present in the <code>config</code> object.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">INFO</dt>
<dd>
<p>The Cluster Operator does not validate the provided configuration.
When invalid configuration is provided, the Kafka Connect cluster might not start or might become unstable.
In such cases, the configuration in the <code>config</code> object should be fixed and the Cluster Operator will roll out the new configuration to all Kafka Connect instances.</p>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="logging_3">Logging</h5>
<div class="paragraph">
<p>The <code>logging</code> field allows the configuration of loggers. These loggers are:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>log4j.rootLogger
connect.root.logger.level
log4j.logger.org.apache.zookeeper
log4j.logger.org.I0Itec.zkclient
log4j.logger.org.reflections</code></pre>
</div>
</div>
<div class="paragraph">
<p>For information on the logging options and examples of how to set logging, see <a href="#logging_examples">logging examples</a> for Kafka.</p>
</div>
</div>
<div class="sect4">
<h5 id="kafka_connect_s2i_deployment">Kafka Connect S2I deployment</h5>
<div class="paragraph">
<p>When using Strimzi together with an OpenShift cluster, a user can deploy Kafka Connect with support for <a href="https://docs.openshift.org/3.9/dev_guide/builds/index.html">OpenShift Builds</a> and <a href="https://docs.openshift.org/3.9/creating_images/s2i.html#creating-images-s2i">Source-to-Image (S2I)</a>.
To activate the S2I deployment a <code>KafkaConnectS2I</code> resource should be used instead of a <code>KafkaConnect</code> resource.
The following is a full example of <code>KafkaConnectS2I</code> resource.</p>
</div>
<div class="listingblock">
<div class="title">Example <code>KafkaConnectS2I</code> resource</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: {KafkaConnectS2I}
kind: KafkaConnectS2I
metadata:
  name: my-connect-cluster
spec:
  nodes: 1
  image: strimzi/kafka-connect-s2i:latest
  readinessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  livenessProbe:
    initialDelaySeconds: 60
    timeoutSeconds: 5
  config:
    bootstrap.servers: my-cluster-kafka:9092</code></pre>
</div>
</div>
<div class="paragraph">
<p>The S2I deployment is very similar to the regular Kafka Connect deployment (as represented by the <code>KafkaConnect</code> resource).
Compared to the regular deployment, the Cluster Operator will create the following additional resources:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">[connect-cluster-name]-connect-source</dt>
<dd>
<p>ImageStream which is used as the base image for the newly-built Docker images.</p>
</dd>
<dt class="hdlist1">[connect-cluster-name]-connect</dt>
<dd>
<p>BuildConfig which is responsible for building the new Kafka Connect Docker images.</p>
</dd>
<dt class="hdlist1">[connect-cluster-name]-connect</dt>
<dd>
<p>ImageStream where the newly built Docker images will be pushed.</p>
</dd>
<dt class="hdlist1">[connect-cluster-name]-connect</dt>
<dd>
<p>DeploymentConfig which is in charge of creating the Kafka Connect worker node pods.</p>
</dd>
<dt class="hdlist1">[connect-cluster-name]-connect</dt>
<dd>
<p>Service which exposes the REST interface for managing the Kafka Connect cluster.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The Kafka Connect S2I deployment supports the same options as the regular Kafka Connect deployment.
A list of supported options can be found in the <a href="#kafka_connect_config_map_details">Kafka Connect</a> section.
The <code>image</code> option specifies the Docker image which will be used as the <em>source image</em> - the base image for the newly built Docker image.
The default value of the <code>image</code> option is determined by the value of the <code><a href="#STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE">STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE</a></code> environment variable of the Cluster Operator.
All other options have the same meaning as for the regular <code>KafkaConnect</code> deployment.</p>
</div>
<div class="paragraph">
<p>Once the Kafka Connect S2I cluster is deployed, new plugins can be added by starting a new OpenShift build.
Before starting the build, a directory with all the KafkaConnect plugins which should be added has to be created.
The plugins and all their dependencies can be in a single directory or can be split into multiple subdirectories.
For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ tree ./s2i-plugins/
./s2i-plugins/
├── debezium-connector-mysql
│   ├── CHANGELOG.md
│   ├── CONTRIBUTE.md
│   ├── COPYRIGHT.txt
│   ├── debezium-connector-mysql-0.7.1.jar
│   ├── debezium-core-0.7.1.jar
│   ├── LICENSE.txt
│   ├── mysql-binlog-connector-java-0.13.0.jar
│   ├── mysql-connector-java-5.1.40.jar
│   ├── README.md
│   └── wkb-1.0.2.jar
└── debezium-connector-postgres
    ├── CHANGELOG.md
    ├── CONTRIBUTE.md
    ├── COPYRIGHT.txt
    ├── debezium-connector-postgres-0.7.1.jar
    ├── debezium-core-0.7.1.jar
    ├── LICENSE.txt
    ├── postgresql-42.0.0.jar
    ├── protobuf-java-2.6.1.jar
    └── README.md</code></pre>
</div>
</div>
<div class="paragraph">
<p>A new build can be started using the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc start-build my-connect-cluster-connect --from-dir ./s2i-plugins/</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will upload the whole directory into the OpenShift cluster and start a new build.
The build will take the base Docker image from the source ImageStream (named <em>[connect-cluster-name]-connect-source</em>) and add the directory and all the files it contains into this image and push the resulting image into the target ImageStream (named <em>[connect-cluster-name]-connect</em>).
When the new image is pushed to the target ImageStream, a rolling update of the Kafka Connect S2I deployment will be started and will roll out the new version of the image with the added plugins.
By default, the <code>oc start-build</code> command will trigger the build and complete.
The progress of the build can be observed in the OpenShift console.
Alternatively, the option <code>--follow</code> can be used to follow the build from the command line:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc start-build my-connect-cluster-connect --from-dir ./s2i-plugins/ --follow
Uploading directory "s2i-plugins" as binary input for the build ...
build "my-connect-cluster-connect-3" started
Receiving source from STDIN as archive ...
Assembling plugins into custom plugin directory /tmp/kafka-plugins
Moving plugins to /tmp/kafka-plugins

Pushing image 172.30.1.1:5000/myproject/my-connect-cluster-connect:latest ...
Pushed 6/10 layers, 60% complete
Pushed 7/10 layers, 70% complete
Pushed 8/10 layers, 80% complete
Pushed 9/10 layers, 90% complete
Pushed 10/10 layers, 100% complete
Push successful</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
The S2I build will always add the additional Kafka Connect plugins to the original source image.
They will not be added to the Docker image from a previous build.
To add multiple plugins to the deployment, they all have to be added within the same build.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="provisioning-rbac-for-the-cluster-operator">3.3. Provisioning Role-Based Access Control (RBAC) for the Cluster Operator</h3>
<div class="paragraph">
<p>For the Cluster Operator to function it needs permission within the OpenShift or Kubernetes cluster to interact with the resources it manages and interacts with.
These include the desired resources, such as  <code>Kafka</code>, <code>Kafka Connect</code>, and so on, as well as the managed resources, such as <code>ConfigMaps</code>, <code>Pods</code>, <code>Deployments</code>, <code>StatefulSets</code>, <code>Services</code>, and so on.
Such permission is described in terms of OpenShift or Kubernetes role-based access control (RBAC) resources:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ServiceAccount</code>,</p>
</li>
<li>
<p><code>Role</code> and <code>ClusterRole</code>,</p>
</li>
<li>
<p><code>RoleBinding</code> and <code>ClusterRoleBinding</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In addition to running under its own <code>ServiceAccount</code> with a <code>ClusterRoleBinding</code>, the Cluster Operator manages some RBAC resources because some of the components need access to OpenShift or Kubernetes resources.</p>
</div>
<div class="paragraph">
<p>OpenShift or Kubernetes also includes privilege escalation protections that prevent components operating under one <code>ServiceAccount</code> from granting other <code>ServiceAccounts</code> privileges that the granting <code>ServiceAccount</code> does not have.
Because the Cluster Operator must be able to create the <code>ClusterRoleBindings</code> and <code>RoleBindings</code> needed by resources it manages, the Cluster Operator must also have those same privileges.</p>
</div>
<div class="sect3">
<h4 id="delegated-privileges">3.3.1. Delegated privileges</h4>
<div class="paragraph">
<p>When the Cluster Operator deploys resources for a desired <code>Kafka</code> resource it also creates <code>ServiceAccounts</code>, <code>RoleBindings</code> and <code>ClusterRoleBindings</code>, as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Kafka broker pods use a <code>ServiceAccount</code> called <code>&lt;cluster-name&gt;-kafka</code> (where <code>&lt;cluster-name&gt;</code> is a placeholder for the name of the <code>Kafka</code> resource)</p>
<div class="ulist">
<ul>
<li>
<p>When the rack feature is used, the <code>strimzi-&lt;cluster-name&gt;-kafka-init</code> <code>ClusterRoleBinding</code> is used to grant this <code>ServiceAccount</code> access to the nodes within the cluster via a <code>ClusterRole</code> called <code>strimzi-kafka-broker</code></p>
</li>
<li>
<p>When the rack feature is not used no binding is created.</p>
</li>
</ul>
</div>
</li>
<li>
<p>The Zookeeper pods use the default <code>ServiceAccount</code> as they have no need to access OpenShift or Kubernetes resources</p>
</li>
<li>
<p>The Topic Operator pod uses a <code>ServiceAccount</code> called <code>&lt;cluster-name&gt;-topic-operator</code> (where <code>&lt;cluster-name&gt;</code> is a placeholder for the name of the <code>Kafka</code> resource)</p>
<div class="ulist">
<ul>
<li>
<p>The Topic Operator produces OpenShift or Kubernetes events with status information, so the <code>ServiceAccount</code> is bound to a <code>ClusterRole</code> called <code>strimzi-topic-operator</code> which grants this access via the <code>strimzi-topic-operator-role-binding</code> <code>RoleBinding</code>.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The pods for <code>KafkaConnect</code> and <code>KafkaConnectS2I</code> resources use the default <code>ServiceAccount</code>, since they require no access to OpenShift or Kubernetes resources.</p>
</div>
</div>
<div class="sect3">
<h4 id="using_a_serviceaccount">3.3.2. Using a <code>ServiceAccount</code></h4>
<div class="paragraph">
<p>The Cluster Operator is best run using a <code>ServiceAccount</code>:</p>
</div>
<div class="listingblock">
<div class="title">Example <code>ServiceAccount</code> for the Cluster Operator</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: strimzi-cluster-operator
  labels:
    app: strimzi</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>Deployment</code> of the operator then needs to specify this in its <code>spec.template.spec.serviceAccountName</code>:</p>
</div>
<div class="listingblock">
<div class="title">Partial example <code>Deployment</code> for the Cluster Operator</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: strimzi-cluster-operator
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: strimzi-cluster-operator
    spec:
      serviceAccountName: strimzi-cluster-operator
      containers:
      # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note line 12, where the the <code>strimzi-cluster-operator</code> <code>ServiceAccount</code> is specified as the <code>serviceAccountName</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="defining_clusterroles">3.3.3. Defining <code>ClusterRoles</code></h4>
<div class="paragraph">
<p>The Cluster Operator needs to operate using <code>ClusterRoles</code> that give it access to the necessary resources.
Depending on the OpenShift or Kubernetes cluster setup, a cluster administrator might be needed to create the <code>ClusterRoles</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
Cluster administrator rights are only needed for the creation of the <code>ClusterRoles</code>.
The Cluster Operator will not run under the cluster admin account.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>ClusterRoles</code> follow the "principle of least privilege" and contain only those privileges needed by the Cluster Operator to operate Kafka, Kafka Connect, and Zookeeper clusters. The first set of assigned privileges allow the Cluster Operator to manage OpenShift or Kubernetes resources such as <code>StatefulSets</code>, <code>Deployments</code>, <code>Pods</code>, and <code>ConfigMaps</code>.</p>
</div>
<div class="listingblock">
<div class="title">Example <code>Role</code> for the Cluster Operator</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: strimzi-cluster-operator
  labels:
    app: strimzi
rules:
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - get
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
  - clusterrolebindings
  - rolebindings
  verbs:
    - get
    - create
    - delete
    - patch
    - update
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - kafkas
  - kafkaconnects
  - kafkaconnects2is
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
  - delete
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - "extensions"
  resources:
  - deployments
  - deployments/scale
  - replicasets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - "apps"
  resources:
  - deployments
  - deployments/scale
  - deployments/status
  - statefulsets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
# OpenShift S2I requirements
- apiGroups:
  - "extensions"
  resources:
  - replicationcontrollers
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - apps.openshift.io
  resources:
  - deploymentconfigs
  - deploymentconfigs/scale
  - deploymentconfigs/status
  - deploymentconfigs/finalizers
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - build.openshift.io
  resources:
  - buildconfigs
  - builds
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - watch
  - update
- apiGroups:
  - image.openshift.io
  resources:
  - imagestreams
  - imagestreams/status
  verbs:
  - create
  - delete
  - get
  - list
  - watch
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - replicationcontrollers
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - list
  - create
  - delete
  - patch
  - update</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>strimzi-kafka-broker</code> <code>ClusterRole</code> represents the access needed by the init container in Kafka pods that is used for the rack feature. As described in the <a href="#delegated-privileges">Delegated privileges</a> section, this role is also needed by the Cluster Operator in order to be able to delegate this access.</p>
</div>
<div class="listingblock">
<div class="title"><code>ClusterRole</code> for the Cluster Operator allowing it to delegate access to OpenShift or Kubernetes nodes to the Kafka broker pods</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml"># These are the privileges needed by the init container
# in Kafka broker pods.
# The Cluster Operator also needs these privileges since
# it binds the Kafka pods' ServiceAccount to this
# role.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: strimzi-kafka-broker
  labels:
    app: strimzi
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>strimzi-topic-operator</code> <code>ClusterRole</code> represents the access needed by the Topic Operator. As described in the <a href="#delegated-privileges">Delegated privileges</a> section, this role is also needed by the Cluster Operator in order to be able to delegate this access.</p>
</div>
<div class="listingblock">
<div class="title"><code>ClusterRole</code> for the Cluster Operator allowing it to delegate access to events to the Topic Operator</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml"># These are the privileges needed by the Topic Operator.
# The Cluster Operator also needs these privileges since
# it binds the Topic Operator's  ServiceAccount to this
# role.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: strimzi-topic-operator
  labels:
    app: strimzi
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="defining_clusterrolebindings">3.3.4. Defining <code>ClusterRoleBindings</code></h4>
<div class="paragraph">
<p>The operator needs a <code>ClusterRoleBinding</code> which associates its <code>ClusterRole</code> with its <code>ServiceAccount</code>:</p>
</div>
<div class="listingblock">
<div class="title">Example <code>RoleBinding</code> for the Cluster Operator</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: strimzi-cluster-operator
  labels:
    app: strimzi
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: myproject
roleRef:
  kind: ClusterRole
  name: strimzi-cluster-operator
  apiGroup: rbac.authorization.k8s.io</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ClusterRoleBindings</code> are also needed for the <code>ClusterRoles</code> needed for delegation:</p>
</div>
<div class="listingblock">
<div class="title">Example <code>RoleBinding</code> for the Cluster Operator</div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: strimzi-cluster-operator-kafka-broker-delegation
  labels:
    app: strimzi
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: myproject
roleRef:
  kind: ClusterRole
  name: strimzi-kafka-broker
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: strimzi-cluster-operator-topic-operator-delegation
  labels:
    app: strimzi
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: myproject
roleRef:
  kind: ClusterRole
  name: strimzi-topic-operator
  apiGroup: rbac.authorization.k8s.io
---</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="operator_configuration">3.4. Operator configuration</h3>
<div class="paragraph">
<p>The operator itself can be configured through the following environment variables.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><a id="STRIMZI_NAMESPACE"></a> <code>STRIMZI_NAMESPACE</code></dt>
<dd>
<p>Required. A comma-separated list of namespaces that the operator should
operate in. The Cluster Operator deployment might use the <a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api">Kubernetes Downward API</a>
to set this automatically to the namespace the Cluster Operator is deployed in. See the example below:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">env:
  - name: STRIMZI_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1"><a id="STRIMZI_FULL_RECONCILIATION_INTERVAL_MS"></a> <code>STRIMZI_FULL_RECONCILIATION_INTERVAL_MS</code></dt>
<dd>
<p>Optional, default: 120000 ms. The interval between periodic reconciliations, in milliseconds.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_OPERATION_TIMEOUT_MS"></a> <code>STRIMZI_OPERATION_TIMEOUT_MS</code></dt>
<dd>
<p>Optional, default: 300000 ms. The timeout for internal operations, in milliseconds. This value should be
increased when using Strimzi on clusters where regular OpenShift or Kubernetes operations take longer than usual (because of slow downloading of Docker images, for example).</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_DEFAULT_KAFKA_IMAGE"></a> <code>STRIMZI_DEFAULT_KAFKA_IMAGE</code></dt>
<dd>
<p>Optional, default <code>strimzi/kafka:latest</code>.
The image name to use as a default when deploying Kafka, if
no image is specified as the <code>kafka-image</code> in the <a href="#kafka_config_map_details">Kafka cluster ConfigMap</a>.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_DEFAULT_KAFKA_INIT_IMAGE"></a> <code>STRIMZI_DEFAULT_KAFKA_INIT_IMAGE</code></dt>
<dd>
<p>Optional, default <code>strimzi/kafka-init:latest</code>.
The image name to use as default for the init container started before the broker for doing initial configuration work (that is, rack support), if no image is specified as the <code>kafka-init-image</code> in the <a href="#kafka_config_map_details">Kafka cluster ConfigMap</a>.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE"></a> <code>STRIMZI_DEFAULT_KAFKA_CONNECT_IMAGE</code></dt>
<dd>
<p>Optional, default <code>strimzi/kafka-connect:latest</code>.
The image name to use as a default when deploying Kafka Connect, if
no image is specified as the <code>image</code> in the
<a href="#kafka_connect_config_map_details">Kafka Connect cluster ConfigMap</a>.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE"></a> <code>STRIMZI_DEFAULT_KAFKA_CONNECT_S2I_IMAGE</code></dt>
<dd>
<p>Optional, default <code>strimzi/kafka-connect-s2i:latest</code>.
The image name to use as a default when deploying Kafka Connect S2I, if
no image is specified as the <code>image</code> in the cluster ConfigMap.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE"></a> <code>STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE</code></dt>
<dd>
<p>Optional, default <code>strimzi/topic-operator:latest</code>.
The image name to use as a default when deploying the topic operator, if
no image is specified as the <code>image</code> in the <a href="#topic_operator_json_config">topic operator config</a>
of the Kafka cluster ConfigMap.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_DEFAULT_ZOOKEEPER_IMAGE"></a> <code>STRIMZI_DEFAULT_ZOOKEEPER_IMAGE</code></dt>
<dd>
<p>Optional, default <code>strimzi/zookeeper:latest</code>.
The image name to use as a default when deploying Zookeeper, if
no image is specified as the <code>zookeeper-image</code> in the <a href="#kafka_config_map_details">Kafka cluster ConfigMap</a>.</p>
</dd>
<dt class="hdlist1"><a id="STRIMZI_LOG_LEVEL"></a> <code>STRIMZI_LOG_LEVEL</code></dt>
<dd>
<p>Optional, default <code>INFO</code>.
The level for printing logging messages. The value can be set to: <code>ERROR</code>, <code>WARNING</code>, <code>INFO</code>, <code>DEBUG</code> and <code>TRACE</code>.</p>
</dd>
</dl>
</div>
<div class="sect3">
<h4 id="multi-namespace">3.4.1. Watching multiple namespaces</h4>
<div class="paragraph">
<p>The <code>STRIMZI_NAMESPACE</code> environment variable can be used to configure a single operator instance
to operate in multiple namespaces. For each namespace given, the operator will watch for cluster ConfigMaps
and perform periodic reconciliation. To be able to do this, the operator&#8217;s ServiceAccount needs
access to the necessary resources in those other namespaces. This can be done by creating an additional
RoleBinding in each of those namespaces, associating the operator&#8217;s ServiceAccount
(<code>strimzi-cluster-operator</code> in the examples) with the operator&#8217;s
Role (<code>strimzi-operator-role</code> in the examples).</p>
</div>
<div class="paragraph">
<p>Suppose, for example, that a operator deployed in namespace <code>foo</code> needs to operate in namespace <code>bar</code>.
The following RoleBinding would grant the necessary permissions:</p>
</div>
<div class="listingblock">
<div class="title">Example RoleBinding for a operator to operate in namespace <code>bar</code></div>
<div class="content">
<pre class="highlightjs highlight nowrap"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator-binding-bar
  namespace: bar
  labels:
    app: strimzi
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: foo
roleRef:
  kind: Role
  name: strimzi-cluster-operator-role
  apiGroup: v1</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="topic_operator_2">4. Topic Operator</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Topic Operator is in charge of managing topics in a Kafka cluster. The Topic Operator is deployed as a process
running inside a OpenShift or Kubernetes cluster.
It can be deployed through the Cluster Operator or "manually" through provided YAML files.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/topic_operator.png" alt="Topic Operator">
</div>
</div>
<div class="paragraph">
<p>The role of the Topic Operator is to keep a set of OpenShift or Kubernetes ConfigMaps describing Kafka topics in-sync with
corresponding Kafka topics.</p>
</div>
<div class="paragraph">
<p>Specifically:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>if a ConfigMap is created, the operator will create the topic it describes</p>
</li>
<li>
<p>if a ConfigMap is deleted, the operator will delete the topic it describes</p>
</li>
<li>
<p>if a ConfigMap is changed, the operator will update the topic it describes</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>And also, in the other direction:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>if a topic is created, the operator will create a ConfigMap describing it</p>
</li>
<li>
<p>if a topic is deleted, the operator will create the ConfigMap describing it</p>
</li>
<li>
<p>if a topic is changed, the operator will update the ConfigMap describing it</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This is beneficial to a OpenShift or Kubernetes centric style of deploying
applications, because it allows you to declare a ConfigMap as part of your
applications deployment and the operator will take care of creating
the topic for you, so your application just needs to deal with producing
and/or consuming from the necessary topics.</p>
</div>
<div class="paragraph">
<p>Should the topic be reconfigured, reassigned to different Kafka nodes and so on,
the ConfigMap will always be up to date.</p>
</div>
<div class="sect2">
<h3 id="reconciliation_2">4.1. Reconciliation</h3>
<div class="paragraph">
<p>A fundamental problem that the operator has to solve is that there is no single source of truth:
Both the ConfigMap and the topic can be modified independently of the operator.
Complicating this, the Topic Operator might not always be able to observe changes at each end in real time (for example, the operator might be down).</p>
</div>
<div class="paragraph">
<p>To resolve this, the operator maintains its own private copy of the
information about each topic.
When a change happens either in the Kafka cluster, or
in OpenShift or Kubernetes, it looks at both the state of the other system, and at its
private copy in order to determine what needs to change to keep everything in sync.
The same thing happens whenever the operator starts, and periodically while its running.</p>
</div>
<div class="paragraph">
<p>For example, suppose the Topic Operator is not running, and a ConfigMap "my-topic" gets created.
When the operator starts it will lack a private copy of "my-topic",
so it can infer that the ConfigMap has been created since it was last running.
The operator will create the topic corresponding to "my-topic" and also store a private copy of the
metadata for "my-topic".</p>
</div>
<div class="paragraph">
<p>The private copy allows the operator to cope with scenarios where the topic config gets changed both in Kafka and in OpenShift or Kubernetes, so long as the changes are not incompatible (for example, both changing the same topic config key, but to different values).
In the case of incompatible changes, the Kafka configuration wins, and the ConfigMap will be updated to reflect that.
Defaulting to the Kafka configuration ensures that, in the worst case, data won&#8217;t be lost.</p>
</div>
<div class="paragraph">
<p>The private copy is held in the same ZooKeeper ensemble used by Kafka itself.
This mitigates availability concerns, because if ZooKeeper is not running
then Kafka itself cannot run, so the operator will be no less available
than it would even if it was stateless.</p>
</div>
</div>
<div class="sect2">
<h3 id="usage_recommendations">4.2. Usage Recommendations</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Try to either always operate on ConfigMaps or always operate directly on topics.</p>
</li>
<li>
<p>When creating a ConfigMap:</p>
<div class="ulist">
<ul>
<li>
<p>Remember that the name cannot be easily changed later.</p>
</li>
<li>
<p>Choose a name for the ConfigMap that reflects the name of the topic it describes.</p>
</li>
<li>
<p>Ideally the ConfigMap&#8217;s <code>metadata.name</code> should be the same as its <code>data.name</code>.
To do this, the topic name will have to be a <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md">valid Kubernetes resource name</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>When creating a topic:</p>
<div class="ulist">
<ul>
<li>
<p>Remember that the name cannot be easily changed later.</p>
</li>
<li>
<p>It is best to use a name that is a <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md">valid Kubernetes resource name</a>,
otherwise the operator will have to sanitize the name when creating
the corresponding ConfigMap.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="topic_config_map_details">4.3. Format of the ConfigMap</h3>
<div class="paragraph">
<p>By default, the operator only considers ConfigMaps having the label <code>strimzi.io/kind=topic</code>,
but this is configurable via the <code>STRIMZI_CONFIGMAP_LABELS</code> environment variable.</p>
</div>
<div class="paragraph">
<p>The <code>data</code> of such ConfigMaps supports the following keys:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>name</code></dt>
<dd>
<p>The name of the topic. Optional; if this is absent the name of the ConfigMap itself is used.</p>
</dd>
<dt class="hdlist1"><code>partitions</code></dt>
<dd>
<p>The number of partitions of the Kafka topic. This can be increased, but not decreased. Required.</p>
</dd>
<dt class="hdlist1"><code>replicas</code></dt>
<dd>
<p>The number of replicas of the Kafka topic. This cannot be larger than the number of nodes in the Kafka cluster. Required.</p>
</dd>
<dt class="hdlist1"><code>config</code></dt>
<dd>
<p>A string in JSON format representing the <a href="https://kafka.apache.org/documentation/#topicconfigs">topic configuration</a>. Optional, defaulting to the empty set.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="example">4.4. Example</h3>
<div class="paragraph">
<p>This example shows how to create a topic called "orders" with 10 partitions and 2 replicas.</p>
</div>
<div class="sect3">
<h4 id="on_kubernetes">4.4.1. On Kubernetes</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The ConfigMap has to be prepared:</p>
<div class="listingblock">
<div class="title">Topic declaration ConfigMap</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: orders
  labels:
    strimzi.io/kind: topic
    strimzi.io/cluster: my-cluster
data:
  name: orders
  partitions: "10"
  replicas: "2"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Because the <code>config</code> key is omitted from the <code>data</code> the topic&#8217;s config will be empty, and thus default to the
Kafka broker default.</p>
</div>
</li>
<li>
<p>The ConfigMap should be created in Kubernetes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl create -f orders-topic.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>In case the topic should be later changed to retention time to 4 days, the <code>orders-topic.yaml</code> file can be updated:</p>
<div class="listingblock">
<div class="title">Topic declaration ConfigMap with "config" update</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: orders
  labels:
    strimzi.io/kind: topic
    strimzi.io/cluster: my-cluster
data:
  name: orders
  partitions: "10"
  replicas: "2"
  config: '{ "retention.ms":"345600000" }'</code></pre>
</div>
</div>
</li>
<li>
<p>The changes in the file have to be applied on Kubernetes using <code>kubectl update -f</code>.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
When the Topic Operator is deployed manually the <code>strimzi.io/cluster</code> label is not necessary.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="on_openshift">4.4.2. On OpenShift</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The ConfigMap has to be prepared:</p>
<div class="listingblock">
<div class="title">Topic declaration ConfigMap</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: orders
  labels:
    strimzi.io/kind: topic
    strimzi.io/cluster: my-cluster
data:
  name: orders
  partitions: "10"
  replicas: "2"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Because the <code>config</code> key is omitted from the <code>data</code> the topic&#8217;s config will be empty, and thus default to the
Kafka broker default.</p>
</div>
</li>
<li>
<p>The ConfigMap should be created in OpenShift:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f orders-topic.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>In case the topic should be later changed to retention time to 4 days, the <code>orders-topic.yaml</code> file can be updated:</p>
<div class="listingblock">
<div class="title">Topic declaration ConfigMap with "config" update</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: orders
  labels:
    strimzi.io/kind: topic
    strimzi.io/cluster: my-cluster
data:
  name: orders
  partitions: "10"
  replicas: "2"
  config: '{ "retention.ms":"345600000" }'</code></pre>
</div>
</div>
</li>
<li>
<p>The changes in the file have to be updated on OpenShift using <code>oc update -f</code>.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
When the Topic Operator is deployed manually the <code>strimzi.io/cluster</code> label is not necessary.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="unsupported_operations">4.5. Unsupported operations</h3>
<div class="ulist">
<ul>
<li>
<p>The <code>data.name</code> cannot be changed key in a ConfigMap, because Kafka does not support changing topic names.</p>
</li>
<li>
<p>The <code>data.partitions</code> cannot be decreased, because Kafka does not support this.</p>
</li>
<li>
<p>Increasing <code>data.partitions</code> for topics with keys should be exercised with caution, as it will change
how records are partitioned.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="operator_environment">4.6. Operator environment</h3>
<div class="paragraph">
<p>The operator is configured from environment variables:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>STRIMZI_CONFIGMAP_LABELS</code>
– The label selector used to identify ConfigMaps to be managed by the operator.
  Default: <code>strimzi.io/kind=topic</code>.</p>
</li>
<li>
<p><code>STRIMZI_ZOOKEEPER_SESSION_TIMEOUT_MS</code>
– The Zookeeper session timeout, in milliseconds.
For example <code>10000</code>.
Default: <code>20000</code> (20 seconds).</p>
</li>
<li>
<p><code>STRIMZI_KAFKA_BOOTSTRAP_SERVERS</code>
– The list of Kafka bootstrap servers.
This variable is mandatory.</p>
</li>
<li>
<p><code>STRIMZI_ZOOKEEPER_CONNECT</code>
– The Zookeeper connection information.
This variable is mandatory.</p>
</li>
<li>
<p><code>STRIMZI_FULL_RECONCILIATION_INTERVAL_MS</code>
– The interval between periodic reconciliations, in milliseconds.</p>
</li>
<li>
<p><code>STRIMZI_TOPIC_METADATA_MAX_ATTEMPTS</code>
– The number of attempts for getting topics metadata from Kafka.
The time between each attempt is defined as an exponential back-off.
You might want to increase this value when topic creation could take more time due to its larger size (that is, many partitions/replicas).
Default <code>6</code>.</p>
</li>
<li>
<p><code>STRIMZI_LOG_LEVEL</code>
– The level for printing logging messages.
The value can be set to: <code>ERROR</code>, <code>WARNING</code>, <code>INFO</code>, <code>DEBUG</code> and <code>TRACE</code>.
Default <code>INFO</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If the operator configuration needs to be changed the process must be killed and restarted.
Since the operator is intended to execute within OpenShift or Kubernetes, this can be achieved
by deleting the pod.</p>
</div>
</div>
<div class="sect2">
<h3 id="resource_limits_and_requests">4.7. Resource limits and requests</h3>
<div class="paragraph">
<p>The Topic Operator can run with resource limits:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>When it is deployed by the Cluster Operator these can be specified in the <code>resources</code> key of the <code>topic-operator-config</code>.</p>
</li>
<li>
<p>When it is not deployed by the Cluster Operator these can be specified on the Deployment in the usual way.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="minimum_resource_requirements_2">4.7.1. Minimum Resource Requirements</h4>
<div class="paragraph">
<p>Testing has shown that the topic operator functions adequately with 96Mi of memory and 100m CPU when watching two topics.
It is therefore recommended to use these as a minimum when configuring resource requests and not to run it with lower
limits than these. If the Kafka cluster has more than a handful of topics more generous requests and limits will be
necessary.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="security">5. Security</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Strimzi project supports data encryption of communication between the different Kafka and Strimzi components (Kafka brokers, Zookeeper nodes, Kafka Connect and Strimzi Topic Operator) by means of the SSL/TLS protocol.
This makes it possible to encrypt data transferred between brokers (interbroker communication) and between clients and brokers.
For the Kafka and Strimzi components, TLS certificates are also used for authentication.</p>
</div>
<div class="paragraph">
<p>The Cluster Operator sets up the SSL/TLS certificates to provide this encryption and authentication.</p>
</div>
<div class="sect2">
<h3 id="certificates">5.1. Certificates</h3>
<div class="paragraph">
<p>Each component needs its own private and public keys in order to support encryption.
The public key has to be signed by a certificate authority (CA) in order to have a related X.509 certificate for providing server authentication and encrypting the communication channel with the client (which could be another broker as well).
All component certificates are signed by a Certification Authority (CA) called <em>cluster CA</em>.
Another CA is used for authentication of Kafka clients connecting to Kafka brokers.
This CA is called <em>clients CA</em>.
The CAs themselves use self-signed certificates.</p>
</div>
<div class="paragraph">
<p>All the generated certificates are saved as Secrets in the OpenShift or Kubernetes cluster, named as follows:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;cluster-name&gt;-cluster-ca</code></dt>
<dd>
<p>Contains the private and public keys of the cluster CA which is used for signing server certificates for the Kafka and Strimzi components (Kafka brokers, Zookeeper nodes, and so on).</p>
</dd>
<dt class="hdlist1"><code>&lt;cluster-name&gt;-cluster-ca-cert</code></dt>
<dd>
<p>Contains only the public key of the cluster CA.
The public key used by Kafka clients to verify the identity of the Kafka brokers they are connecting to (TLS server authentication).</p>
</dd>
<dt class="hdlist1"><code>&lt;cluster-name&gt;-clients-ca</code></dt>
<dd>
<p>Contains the private and public keys of the clients CA.
The clients CA is used for TLS client authentication of Kafka clients when connecting to Kafka brokers.</p>
</dd>
<dt class="hdlist1"><code>&lt;cluster-name&gt;-clients-ca-cert</code></dt>
<dd>
<p>Contains only the public key of the client CA.
The public key is used by the Kafka brokers to verify the identity of Kafka clients when TLS client authentication is used.</p>
</dd>
<dt class="hdlist1"><code>&lt;cluster-name&gt;-kafka-brokers</code></dt>
<dd>
<p>Contains all the Kafka broker private and public keys (certificates signed with the cluster CA).</p>
</dd>
<dt class="hdlist1"><code>&lt;cluster-name&gt;-zookeeper-nodes</code></dt>
<dd>
<p>Contains all the Zookeeper node private and public keys (certificates signed with the cluster CA).</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>All the keys are 2048 bits in size and are valid for 365 days from initial generation.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
"certificates rotation" for generating new ones on their expiration will be supported in future releases.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="kafka_listeners">5.2. Kafka Listeners</h3>
<div class="paragraph">
<p>The data encryption is provided on two different listeners exposed by each Kafka broker.</p>
</div>
<div class="paragraph">
<p>Communication between brokers and between the different Kafka and Strimzi components is done through the <code>REPLICATION</code> listener on port 9091, which is encrypted by default.</p>
</div>
<div class="paragraph">
<p>Encrypted communication with clients is provided through the <code>CLIENTTLS</code> listener on port 9093.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
You can use the <code>CLIENT</code> listener on port 9092 for unencrypted communication with clients.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="kafka_client_connections_via_tls">5.3. Kafka Client connections via TLS</h3>
<div class="paragraph">
<p>If a Kafka client wants to connect to the encrypted listener (<code>CLIENTTLS</code>) on port 9093, it needs to trust the cluster CA certificate in order to verify the broker certificate received during the SSL/TLS handshake.
The cluster CA certificate can be extracted from the generated <code>&lt;cluster-name&gt;-cluster-ca-cert</code> <code>Secret</code>.</p>
</div>
<div class="paragraph">
<p>On Kubernetes, the certificate can be extracted with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl get secret &lt;cluster-name&gt;-cluster-ca-cert -o jsonpath='{.data.ca\.crt}' | base64 -d &gt; ca.crt</code></pre>
</div>
</div>
<div class="paragraph">
<p>On OpenShift, the certificate can be extracted with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get secret &lt;cluster-name&gt;-cluster-ca-cert -o jsonpath='{.data.ca\.crt}' | base64 -d &gt; ca.crt</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Kafka client has to be configured to trust certificates signed by this CA.
For the Java-based Kafka Producer, Consumer and Streams APIs, you can do this by importing the CA certificate into the JVM&#8217;s truststore using the following keytool command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">keytool -keystore client.truststore.jks -alias CARoot -import -file ca.crt</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, in order to configure the Kafka client, following properties should be specified:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>security.protocol</code>: SSL is the value for using encryption.</p>
</li>
<li>
<p><code>ssl.truststore.location</code>: the truststore location where the certificates were imported.</p>
</li>
<li>
<p><code>ssl.truststore.password</code>: password for accessing the truststore. This property can be omitted if it is not needed by the truststore.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The current implementation does not support Subject Alternative Names (SAN) so the hostname verification should be disabled on the client side.
For doing so the <code>ssl.endpoint.identification.algorithm</code> property needs to be set as empty.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="frequently_asked_questions">Appendix A: Frequently Asked Questions</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="cluster_operator_3">A.1. Cluster Operator</h3>
<div class="sect3">
<h4 id="log_contains_warnings_about_failing_to_acquire_lock">A.1.1. Log contains warnings about failing to acquire lock</h4>
<div class="paragraph">
<p>For each cluster, the Cluster Operator always executes only one operation at a time. The Cluster Operator uses locks
to make sure that there are never two parallel operations running for the same cluster. In case an operation requires
more time to complete, other operations will wait until it is completed and the lock is released.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">INFO</dt>
<dd>
<p>Examples of cluster operations are <em>cluster creation</em>, <em>rolling update</em>, <em>scale down</em> or <em>scale up</em> and so on.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>If the wait for the lock takes too long, the operation times out and the following warning message will be printed to
the log:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">2018-03-04 17:09:24 WARNING AbstractClusterOperations:290 - Failed to acquire lock for kafka cluster lock::kafka::myproject::my-cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>Depending on the exact configuration of <code>STRIMZI_FULL_RECONCILIATION_INTERVAL_MS</code> and <code>STRIMZI_OPERATION_TIMEOUT_MS</code>, this
warning message may appear regularly without indicating any problems. The operations which time out will be picked up by
the next periodic reconciliation. It will try to acquire the lock again and execute.</p>
</div>
<div class="paragraph">
<p>Should this message appear periodically even in situations when there should be no other operations running for a given
cluster, it might indicate that due to some error the lock was not properly released. In such cases it is recommended to
restart the cluster operator.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="installing_kubernetes_and_openshift_cluster">Appendix B: Installing OpenShift or Kubernetes cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The easiest way to get started with OpenShift or Kubernetes is using the <code>Minikube</code>, <code>Minishift</code> or <code>oc cluster up</code>
utilities. This section provides basic guidance on how to use them. More details are provided on the websites of
the tools themselves.</p>
</div>
<div class="sect2">
<h3 id="kubernetes">B.1. Kubernetes</h3>
<div class="paragraph">
<p>In order to interact with a Kubernetes cluster the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/"><code>kubectl</code></a>
utility needs to be installed.</p>
</div>
<div class="paragraph">
<p>The easiest way to get a running Kubernetes cluster is using <code>Minikube</code>. <code>Minikube</code> can be downloaded and installed
from the <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Kubernetes website</a>. Depending on the number of brokers
you want to deploy inside the cluster and if you need Kafka Connect running as well, it could be worth running <code>Minikube</code>
at least with 4 GB of RAM instead of the default 2 GB.
Once installed, it can be started using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">minikube start --memory 4096</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="openshift">B.2. OpenShift</h3>
<div class="paragraph">
<p>In order to interact with an OpenShift cluster, the <a href="https://github.com/openshift/origin/releases"><code>oc</code></a> utility is needed.</p>
</div>
<div class="paragraph">
<p>An OpenShift cluster can be started in two different ways. The <code>oc</code> utility can start a cluster locally using the
command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc cluster up</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command requires Docker to be installed. More information about this way can be found
<a href="https://github.com/openshift/origin/blob/master/docs/cluster_up_down.md">here</a>.</p>
</div>
<div class="paragraph">
<p>Another option is to use <code>Minishift</code>. <code>Minishift</code> is an OpenShift installation within a VM. It can be downloaded and
installed from the <a href="https://docs.openshift.org/latest/minishift/index.html">Minishift website</a>. Depending on the number of brokers
you want to deploy inside the cluster and if you need Kafka Connect running as well, it could be worth running <code>Minishift</code>
at least with 4 GB of RAM instead of the default 2 GB.
Once installed, <code>Minishift</code> can be started using the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">minishift start --memory 4GB</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="api_reference">Appendix C: Custom Resource API Reference</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="type-KafkaAssembly">C.1. <code>Kafka</code> kind v1alpha1 kafka.strimzi.io</h3>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spec</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-KafkaAssemblySpec"><code>KafkaAssemblySpec</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-KafkaAssemblySpec">C.2. <code>KafkaAssemblySpec</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#kind-Kafka"><code>KafkaAssembly</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kafka</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Configuration of the Kafka cluster</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Kafka"><code>Kafka</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">zookeeper</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Configuration of the Zookeeper cluster</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Zookeeper"><code>Zookeeper</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">topicOperator</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Configuration of the Topic Operator</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-TopicOperator"><code>TopicOperator</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-Kafka">C.3. <code>Kafka</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-KafkaAssemblySpec"><code>KafkaAssemblySpec</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">replicas</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The number of pods in the cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">image</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The docker image for the pods.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">storage</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Storage configuration (disk). Cannot be updated. The type depends on the value of the <code>storage.type</code> property within the given object, which must be one of [ephemeral, persistent-claim]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-EphemeralStorage"><code>EphemeralStorage</code></a>, <a href="#type-PersistentClaimStorage"><code>PersistentClaimStorage</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">brokerRackInitImage</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The image of the init container used for initializing the <code>broker.rack</code>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">livenessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod liveness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">readinessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod readiness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">jvmOptions</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">JVM Options for pods</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-JvmOptions"><code>JvmOptions</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">affinity</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod affinity rules.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">core/v1 affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">Affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">metrics</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The Prometheus JMX Exporter configuration. See <a href="https://github.com/prometheus/jmx_exporter" class="bare">https://github.com/prometheus/jmx_exporter</a> for details of the structure of this configuration.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tlsSidecar</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">TLS sidecar configuration</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Sidecar"><code>Sidecar</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">config</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The kafka broker config. Properties with the following prefixes cannot be set: listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl., security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer., super.user</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">logging</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Logging configuration for Kafka The type depends on the value of the <code>logging.type</code> property within the given object, which must be one of [inline, external]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-InlineLogging"><code>InlineLogging</code></a>, <a href="#type-ExternalLogging"><code>ExternalLogging</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">rack</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Configuration of the <code>broker.rack</code> broker config.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Rack"><code>Rack</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resources</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource constraints (limits and requests).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Resources"><code>Resources</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tolerations</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod&#8217;s tolerations.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">core/v1 tolerations</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">Toleration</a> array</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-EphemeralStorage">C.4. <code>EphemeralStorage</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<div class="paragraph">
<p>The <code>type</code> property is a discriminator that distinguishes the use of the type <code>EphemeralStorage</code> from <a href="#type-PersistentClaimStorage"><code>PersistentClaimStorage</code></a>.
It must have the value <code>ephemeral</code> for the type <code>EphemeralStorage</code>.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">type</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Must be <code>ephemeral</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-PersistentClaimStorage">C.5. <code>PersistentClaimStorage</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<div class="paragraph">
<p>The <code>type</code> property is a discriminator that distinguishes the use of the type <code>PersistentClaimStorage</code> from <a href="#type-EphemeralStorage"><code>EphemeralStorage</code></a>.
It must have the value <code>persistent-claim</code> for the type <code>PersistentClaimStorage</code>.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">class</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The storage class to use for dynamic volume allocation.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">deleteClaim</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Specifies if the persistent volume claim has to be deleted when the cluster is un-deployed.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">boolean</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">selector</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Specifies a specific persistent volume to use. It contains a matchLabels field which defines an inner JSON object with key:value representing labels for selecting such a volume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">size</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">When type=persistent-claim, defines the size of the persistent volume claim (i.e 1Gi). Mandatory when type=persistent-claim.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">type</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Must be <code>persistent-claim</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-Probe">C.6. <code>Probe</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-KafkaConnectAssemblySpec"><code>KafkaConnectAssemblySpec</code></a>, <a href="#type-KafkaConnectS2IAssemblySpec"><code>KafkaConnectS2IAssemblySpec</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">initialDelaySeconds</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The initial delay before first the health is first checked.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">timeoutSeconds</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The timeout for each attempted health check.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-JvmOptions">C.7. <code>JvmOptions</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-KafkaConnectAssemblySpec"><code>KafkaConnectAssemblySpec</code></a>, <a href="#type-KafkaConnectS2IAssemblySpec"><code>KafkaConnectS2IAssemblySpec</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-XX</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">A map of -XX options to the JVM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-Xms</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">-Xms option to to the JVM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-Xmx</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">-Xmx option to to the JVM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-server</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">-server option to to the JVM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">boolean</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-Sidecar">C.8. <code>Sidecar</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-TopicOperator"><code>TopicOperator</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">image</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The docker image for the container</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resources</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource constraints (limits and requests).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Resources"><code>Resources</code></a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-Resources">C.9. <code>Resources</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-KafkaConnectAssemblySpec"><code>KafkaConnectAssemblySpec</code></a>, <a href="#type-KafkaConnectS2IAssemblySpec"><code>KafkaConnectS2IAssemblySpec</code></a>, <a href="#type-Sidecar"><code>Sidecar</code></a>, <a href="#type-TopicOperator"><code>TopicOperator</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">limits</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource limits applied at runtime.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-CpuMemory"><code>CpuMemory</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">requests</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource requests applied during pod scheduling.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-CpuMemory"><code>CpuMemory</code></a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-CpuMemory">C.10. <code>CpuMemory</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Resources"><code>Resources</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">cpu</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">CPU</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">memory</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Memory</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-InlineLogging">C.11. <code>InlineLogging</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-KafkaConnectAssemblySpec"><code>KafkaConnectAssemblySpec</code></a>, <a href="#type-KafkaConnectS2IAssemblySpec"><code>KafkaConnectS2IAssemblySpec</code></a>, <a href="#type-TopicOperator"><code>TopicOperator</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<div class="paragraph">
<p>The <code>type</code> property is a discriminator that distinguishes the use of the type <code>InlineLogging</code> from <a href="#type-ExternalLogging"><code>ExternalLogging</code></a>.
It must have the value <code>inline</code> for the type <code>InlineLogging</code>.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">loggers</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">type</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-ExternalLogging">C.12. <code>ExternalLogging</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a>, <a href="#type-KafkaConnectAssemblySpec"><code>KafkaConnectAssemblySpec</code></a>, <a href="#type-KafkaConnectS2IAssemblySpec"><code>KafkaConnectS2IAssemblySpec</code></a>, <a href="#type-TopicOperator"><code>TopicOperator</code></a>, <a href="#type-Zookeeper"><code>Zookeeper</code></a></p>
</div>
<div class="paragraph">
<p>The <code>type</code> property is a discriminator that distinguishes the use of the type <code>ExternalLogging</code> from <a href="#type-InlineLogging"><code>InlineLogging</code></a>.
It must have the value <code>external</code> for the type <code>ExternalLogging</code>.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">name</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">type</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-Rack">C.13. <code>Rack</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-Kafka"><code>Kafka</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">topologyKey</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">A key that matches labels assigned to the OpenShift or Kubernetes cluster nodes. The value of the label is used to set the broker&#8217;s <code>broker.rack</code> config.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-Zookeeper">C.14. <code>Zookeeper</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-KafkaAssemblySpec"><code>KafkaAssemblySpec</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">replicas</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The number of pods in the cluster.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">image</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The docker image for the pods.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">storage</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Storage configuration (disk). Cannot be updated. The type depends on the value of the <code>storage.type</code> property within the given object, which must be one of [ephemeral, persistent-claim]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-EphemeralStorage"><code>EphemeralStorage</code></a>, <a href="#type-PersistentClaimStorage"><code>PersistentClaimStorage</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">livenessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod liveness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">readinessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod readiness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">jvmOptions</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">JVM Options for pods</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-JvmOptions"><code>JvmOptions</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">affinity</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod affinity rules.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">core/v1 affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">Affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">metrics</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The Prometheus JMX Exporter configuration. See <a href="https://github.com/prometheus/jmx_exporter" class="bare">https://github.com/prometheus/jmx_exporter</a> for details of the structure of this configuration.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tlsSidecar</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">TLS sidecar configuration</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Sidecar"><code>Sidecar</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">config</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The zookeeper broker config. Properties with the following prefixes cannot be set: server., dataDir, dataLogDir, clientPort, authProvider, quorum.auth, requireClientAuthScheme</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">logging</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Logging configuration for Zookeeper The type depends on the value of the <code>logging.type</code> property within the given object, which must be one of [inline, external]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-InlineLogging"><code>InlineLogging</code></a>, <a href="#type-ExternalLogging"><code>ExternalLogging</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resources</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource constraints (limits and requests).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Resources"><code>Resources</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tolerations</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod&#8217;s tolerations.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">core/v1 tolerations</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">Toleration</a> array</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-TopicOperator">C.15. <code>TopicOperator</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#type-KafkaAssemblySpec"><code>KafkaAssemblySpec</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">watchedNamespace</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The namespace the Topic Operator should watch.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">image</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The image to use for the topic operator</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">reconciliationIntervalSeconds</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Interval between periodic reconciliations.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">zookeeperSessionTimeoutSeconds</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Timeout for the Zookeeper session</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">affinity</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod affinity rules.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">core/v1 affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">Affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resources</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Resources"><code>Resources</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">topicMetadataMaxAttempts</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The number of attempts at getting topic metadata</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tlsSidecar</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">TLS sidecar configuration</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Sidecar"><code>Sidecar</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">logging</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Logging configuration The type depends on the value of the <code>logging.type</code> property within the given object, which must be one of [inline, external]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-InlineLogging"><code>InlineLogging</code></a>, <a href="#type-ExternalLogging"><code>ExternalLogging</code></a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-KafkaConnectAssembly">C.16. <code>KafkaConnect</code> kind v1alpha1 kafka.strimzi.io</h3>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spec</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-KafkaConnectAssemblySpec"><code>KafkaConnectAssemblySpec</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-KafkaConnectAssemblySpec">C.17. <code>KafkaConnectAssemblySpec</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#kind-KafkaConnect"><code>KafkaConnectAssembly</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">replicas</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">image</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The docker image for the pods.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">livenessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod liveness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">readinessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod readiness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">jvmOptions</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">JVM Options for pods</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-JvmOptions"><code>JvmOptions</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">affinity</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod affinity rules.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">core/v1 affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">Affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">metrics</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The Prometheus JMX Exporter configuration. See <a href="https://github.com/prometheus/jmx_exporter" class="bare">https://github.com/prometheus/jmx_exporter</a> for details of the structure of this configuration.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">config</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The Kafka Connect configuration. Properties with the following prefixes cannot be set: ssl., sasl., security., listeners, plugin.path, rest.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">logging</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Logging configuration for Kafka Connect The type depends on the value of the <code>logging.type</code> property within the given object, which must be one of [inline, external]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-InlineLogging"><code>InlineLogging</code></a>, <a href="#type-ExternalLogging"><code>ExternalLogging</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resources</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource constraints (limits and requests).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Resources"><code>Resources</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tolerations</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod&#8217;s tolerations.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">core/v1 tolerations</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">Toleration</a> array</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-KafkaConnectS2IAssembly">C.18. <code>KafkaConnectS2I</code> kind v1alpha1 kafka.strimzi.io</h3>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">spec</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-KafkaConnectS2IAssemblySpec"><code>KafkaConnectS2IAssemblySpec</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="type-KafkaConnectS2IAssemblySpec">C.19. <code>KafkaConnectS2IAssemblySpec</code> type v1alpha1 kafka.strimzi.io</h3>
<div class="paragraph">
<p>Used in: <a href="#kind-KafkaConnectS2I"><code>KafkaConnectS2IAssembly</code></a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Field</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">replicas</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">integer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">image</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The docker image for the pods.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">livenessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod liveness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">readinessProbe</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod readiness checking.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Probe"><code>Probe</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">jvmOptions</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">JVM Options for pods</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-JvmOptions"><code>JvmOptions</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">affinity</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod affinity rules.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">core/v1 affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#affinity-v1-core">Affinity</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">metrics</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The Prometheus JMX Exporter configuration. See <a href="https://github.com/prometheus/jmx_exporter" class="bare">https://github.com/prometheus/jmx_exporter</a> for details of the structure of this configuration.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">additionalProperties</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">config</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">The Kafka Connect configuration. Properties with the following prefixes cannot be set: ssl., sasl., security., listeners, plugin.path, rest.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">map</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">insecureSourceRepository</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">boolean</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">logging</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Logging configuration for Kafka Connect The type depends on the value of the <code>logging.type</code> property within the given object, which must be one of [inline, external]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-InlineLogging"><code>InlineLogging</code></a>, <a href="#type-ExternalLogging"><code>ExternalLogging</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resources</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Resource constraints (limits and requests).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="#type-Resources"><code>Resources</code></a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">tolerations</p></td>
<td class="tableblock halign-left valign-top" rowspan="2"><p class="tableblock">Pod&#8217;s tolerations.See external documentation of <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">core/v1 tolerations</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#tolerations-v1-core">Toleration</a> array</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="metrics_2">Appendix D: Metrics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section describes how to deploy a Prometheus server for scraping metrics from the Kafka cluster and showing them using a Grafana dashboard. The resources provided are examples to show how Kafka metrics can be stored in Prometheus: They are not a recommended configuration, and further support should be available from the Prometheus and Grafana communities.</p>
</div>
<div class="paragraph">
<p>When adding Prometheus and Grafana servers to an Apache Kafka deployment using <code>minikube</code> or <code>minishift</code>, the memory available to the virtual machine should be increased (to 4 GB of RAM, for example, instead of the default 2 GB). Information on how to increase the default amount of memory can be found in the following section <a href="#installing_kubernetes_and_openshift_cluster">Installing OpenShift or Kubernetes cluster</a>.</p>
</div>
<div class="sect2">
<h3 id="deploying_on_openshift">D.1. Deploying on OpenShift</h3>
<div class="sect3">
<h4 id="prometheus">D.1.1. Prometheus</h4>
<div class="paragraph">
<p>The Prometheus server configuration uses a service discovery feature in order to discover the pods in the cluster from which it gets metrics.
In order to have this feature working, it is necessary for the service account used for running the Prometheus service pod to have access to the API server to get the pod list. By default the service account <code>prometheus-server</code> is used.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">export NAMESPACE=[namespace]
oc login -u system:admin
oc create sa prometheus-server
oc adm policy add-cluster-role-to-user cluster-reader system:serviceaccount:${NAMESPACE}:prometheus-server
oc login -u developer</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>[namespace]</code> is the namespace/project where the Apache Kafka cluster was deployed.</p>
</div>
<div class="paragraph">
<p>Finally, create the Prometheus service by running</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/prometheus/kubernetes.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="grafana">D.1.2. Grafana</h4>
<div class="paragraph">
<p>A Grafana server is necessary only to get a visualisation of the Prometheus metrics.</p>
</div>
<div class="paragraph">
<p>To deploy Grafana on OpenShift, the following commands should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc create -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/grafana/kubernetes.yaml</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="deploying_on_kubernetes">D.2. Deploying on Kubernetes</h3>
<div class="sect3">
<h4 id="prometheus_2">D.2.1. Prometheus</h4>
<div class="paragraph">
<p>The Prometheus server configuration uses a service discovery feature in order to discover the pods in the cluster from which it gets metrics.
If the RBAC is enabled in your Kubernetes deployment then in order to have this feature working, it is necessary for the service account used for running the Prometheus service pod to have access to the API server to get the pod list. By default the service account <code>prometheus-server</code> is used.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">export NAMESPACE=[namespace]
kubectl create sa prometheus-server
kubectl create -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/prometheus/cluster-reader.yaml
kubectl create clusterrolebinding read-pods-binding --clusterrole=cluster-reader --serviceaccount=${NAMESPACE}:prometheus-server</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>[namespace]</code> is the namespace/project where the Apache Kafka cluster was deployed.</p>
</div>
<div class="paragraph">
<p>Finally, create the Prometheus service by running</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/prometheus/kubernetes.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="grafana_2">D.2.2. Grafana</h4>
<div class="paragraph">
<p>A Grafana server is necessary only to get a visualisation of Prometheus metrics.</p>
</div>
<div class="paragraph">
<p>To deploy Grafana on Kubernetes, the following commands should be executed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/grafana/kubernetes.yaml</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="grafana_dashboard">D.3. Grafana dashboard</h3>
<div class="paragraph">
<p>As an example, and in order to visualize the exported metrics in Grafana, the simple dashboard <a href="https://github.com/strimzi/strimzi-kafka-operator/blob/master/metrics/examples/grafana/kafka-dashboard.json"><code>kafka-dashboard.json</code></a> file is provided.
The Prometheus data source, and the above dashboard, can be set up in Grafana by following these steps.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
For accessing the dashboard, you can use the <code>port-forward</code> command for forwarding traffic from the Grafana pod to the host. For example, you can access the Grafana UI by running <code>oc port-forward grafana-1-fbl7s 3000:3000</code> (or using <code>kubectl</code> instead of <code>oc</code>) and then pointing a browser to <code><a href="http://localhost:3000" class="bare">http://localhost:3000</a></code>.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Access to the Grafana UI using <code>admin/admin</code> credentials.</p>
<div class="imageblock">
<div class="content">
<img src="images/grafana_login.png" alt="Grafana login">
</div>
</div>
</li>
<li>
<p>Click on the "Add data source" button from the Grafana home in order to add Prometheus as data source.</p>
<div class="imageblock">
<div class="content">
<img src="images/grafana_home.png" alt="Grafana home">
</div>
</div>
</li>
<li>
<p>Fill in the information about the Prometheus data source, specifying a name and "Prometheus" as type. In the URL field, the connection string to the Prometheus server (that is, <code><a href="http://prometheus:9090" class="bare">http://prometheus:9090</a></code>) should be specified. After "Add" is clicked, Grafana will test the connection to the data source.</p>
<div class="imageblock">
<div class="content">
<img src="images/grafana_prometheus_data_source.png" alt="Add Prometheus data source">
</div>
</div>
</li>
<li>
<p>From the top left menu, click on "Dashboards" and then "Import" to open the "Import Dashboard" window where the provided <a href="https://github.com/strimzi/strimzi-kafka-operator/blob/master/metrics/examples/grafana/kafka-dashboard.json"><code>kafka-dashboard.json</code></a> file can be imported or its content pasted.</p>
<div class="imageblock">
<div class="content">
<img src="images/grafana_import_dashboard.png" alt="Add Grafana dashboard">
</div>
</div>
</li>
<li>
<p>After importing the dashboard, the Grafana home should show with some initial metrics about CPU and JVM memory usage. When the Kafka cluster is used (creating topics and exchanging messages) the other metrics, like messages in and bytes in/out per topic, will be shown.</p>
<div class="imageblock">
<div class="content">
<img src="images/grafana_kafka_dashboard.png" alt="Kafka dashboard">
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>